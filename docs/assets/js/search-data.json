{"0": {
    "doc": "Attributions",
    "title": "Attributions",
    "content": ". | https://github.com/lokesh-coder/pretty-checkbox | https://github.com/leongersen/noUiSlider | https://github.com/pure-css/pure/ | https://github.com/fabricjs/fabric.js | . ",
    "url": "/attributions/",
    "relUrl": "/attributions/"
  },"1": {
    "doc": "Bytes, Bits, and Nibbles",
    "title": "Bytes, Bits, and Nibbles",
    "content": "Stub. ",
    "url": "/reference/bytes/",
    "relUrl": "/reference/bytes/"
  },"2": {
    "doc": "Compression Ratios",
    "title": "Compression Ratios",
    "content": "Stub. https://en.wikipedia.org/wiki/Data_compression_ratio . ",
    "url": "/reference/compression_ratios/",
    "relUrl": "/reference/compression_ratios/"
  },"3": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": "This site is intended to be used as a living documentation that is constantly updated by the community, if you feel like something’s missing or would like to contribute, please do one of the following create an issue or create a pull request. The best way to ask a question is to file an issue and we’ll try to get back to you. ",
    "url": "/contributing/",
    "relUrl": "/contributing/"
  },"4": {
    "doc": "File Encodings",
    "title": "File Encodings",
    "content": "Stub. ",
    "url": "/reference/encodings/",
    "relUrl": "/reference/encodings/"
  },"5": {
    "doc": "Fullscreen LZ77/LZSS",
    "title": "Fullscreen LZ77/LZSS",
    "content": "The fullscreen version: . | Does not resize content to fit your canvas | Allows you to pan and zoom | Has a larger canvas window | . ",
    "url": "/interactive/fullscreen_lz/",
    "relUrl": "/interactive/fullscreen_lz/"
  },"6": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "This article is currently a stub. ",
    "url": "/getting_started/",
    "relUrl": "/getting_started/"
  },"7": {
    "doc": "Huffman",
    "title": "Huffman",
    "content": "Since it’s creation by David A. Huffman in 1952, Huffman coding has been regarded as one of the most efficient and optimal methods of compression. Huffman’s optimal compression ratios are made possible through it’s character counting functionality. Unlike many algorithims in the Lempel-Ziv suite, Huffman encoders scan the file and generate a frequency table and tree before begining the true compression process. Before discussing different implementations, lets dive deeper into how the algorithim works. ",
    "url": "/algorithms/huffman/",
    "relUrl": "/algorithms/huffman/"
  },"8": {
    "doc": "Huffman",
    "title": "The Algorithm",
    "content": "Although huffman encoding may seem confusing from an outside view, we can break it into three simple steps: . | Frequency Counting | Tree Building | Character Encoding | . ",
    "url": "/algorithms/huffman/#the-algorithm",
    "relUrl": "/algorithms/huffman/#the-algorithm"
  },"9": {
    "doc": "Huffman",
    "title": "Frequency Countinig",
    "content": "Let’s start out by going over the frequency counting step. Throughout all of the examples, I will be using the following sample input string: . I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! . The huffman encoder starts out by going over the inputed text and outputing a table correlating each character to the number of time it appears in the text. For the sample input, the table would look this: . | Frequency | Character | . | 1 | N | . | 1 | \\n | . | 1 | K | . | 1 | D | . | 1 | L | . | 1 | E | . | 2 | O | . | 3 | H | . | 3 | ! | . | 3 | . | . | 6 | - | . | 6 | S | . | 7 | T | . | 8 | I | . | 12 | M | . | 15 | A | . As displayed above, the table is sorted to ensure consistency in each step of the compression process. ",
    "url": "/algorithms/huffman/#frequency-countinig",
    "relUrl": "/algorithms/huffman/#frequency-countinig"
  },"10": {
    "doc": "Overview",
    "title": "The Hitchhiker’s Guide to Compression",
    "content": "Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape- descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea. This planet has - or rather had - a problem, which was this: files were too big. Many solutions were suggested for solving this problem via lossless compression, such as Lempel-Ziv and Huffman coding, but most of these were implemented into common compression utilities and promptly forgotten. Today, much of the relevant work to compression is in an obscure corner of the internet between lengthy PhD thesis papers and hard-to-find gems. ",
    "url": "/#the-hitchhikers-guide-to-compression",
    "relUrl": "/#the-hitchhikers-guide-to-compression"
  },"11": {
    "doc": "Overview",
    "title": "Why compression",
    "content": "Lossless file compression, and file compression in general has become a lost art. The modern developer community has moved on from working on compression algorithms to bigger and better problems, such as creating the next major NodeJS framework. However, compression as it stands in the computer science aspect is still as interesting as it was in 1980s, possibly even more so today with an estimated 463 Exabytes of data to be created everyday in 2025. It’s no secret that the internet is growing rapidly, and with it more people are becoming connected. From urban dwellers to rural farmers, fast internet speed are not a given. To counter this, there are numerous projects focused on improving internet speeds for rural users, but there are almost no projects focused on the other half of improving internet access: compressing data. These claims about the “lost of art of compression” may seem a bit unsubstantiated, as there are new and actively developed compression projects out there today, such as, but not limited to: . | Facebook’s ZSTD | Google’s Brotli | LZ4 | Shrynk | . However this argument still holds true, compression isn’t really mainstream, and I don’t know why it isn’t. Internet speeds is a real problem and better compression stands as a promising solution. The possibilities of better compression are truly endless: . | Faster 4k video streaming | Faster app downloads | Less delay loading websites and content | and more | . ",
    "url": "/#why-compression",
    "relUrl": "/#why-compression"
  },"12": {
    "doc": "Overview",
    "title": "The Goal",
    "content": "The goal of this project, and by extension, the goal of all resources here is to help people learn about compression algorithms and encourage people to tinker, build, and experiment with their own algorithms and implementations. Afterall, the best way to innovate in tech is to get a bunch of developers interested in something and let them lead the way. Additionally, this project itself is intended to be a community-sourced resource for people interested in compression algorithms. The idea is that anyone can contribute to this website through GitHub so that this can be a constantly improving and expanding resource for others. With all of that said, if you’re interested in learning more about the world of compression, you should get started. ",
    "url": "/#the-goal",
    "relUrl": "/#the-goal"
  },"13": {
    "doc": "Overview",
    "title": "Notable Compression Project Mentions",
    "content": "There are also some other notable projects which I’ve included at the end, but either they aren’t active enough or univeral to be included here. Notable mentions: . | LZHAM (not super active) | Google’s WebP (only for images) | Dropbox’s DivANS | Dropbox’s avrecode | H.266/VCC Codec (domain-specific for video) | Pied Piper Middle-Out (abandoned and closed-source unfortunately) | . ",
    "url": "/#notable-compression-project-mentions",
    "relUrl": "/#notable-compression-project-mentions"
  },"14": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"15": {
    "doc": "Interactive Algorithms",
    "title": "Interactive Algorithms",
    "content": "Stub. ",
    "url": "/interactive/interactive/",
    "relUrl": "/interactive/interactive/"
  },"16": {
    "doc": "LZ77/LZSS",
    "title": "LZ77/LZSS",
    "content": " ",
    "url": "/interactive/lz/",
    "relUrl": "/interactive/lz/"
  },"17": {
    "doc": "Lempel-Ziv",
    "title": "Lempel-Ziv",
    "content": "Lempel-Ziv, commonly referred to as LZ77/LZ78 depending on the variant, is one of the oldest, most simplistic, and widespread compression algorithms out there. It’s power comes from its simplicity, speed, and decent compression rates. Now before we dive into an implementation, let’s understand the concept behind Lempel-Ziv and the various algorithms it has spawned. ",
    "url": "/algorithms/lz/",
    "relUrl": "/algorithms/lz/"
  },"18": {
    "doc": "Lempel-Ziv",
    "title": "The Algorithm(s)",
    "content": "Lempel-Ziv at its core is very simple. It works by taking an input string of characters, finding repetitive characters, and outputting an “encoded” version. To get an idea of it, here’s an example. Original: Hello everyone! Hello world! Encoded: Hello everyone! &lt;16,6&gt;world! . As you can see, the algorithm simply takes an input string, in this case, “Hello everyone! Hello world!”, and encodes it character by character. If it tries to encode a character it has already seen it will check to see if it has seen the next character. This repeats until it the character it’s checking hasn’t been seen before, following the characters it’s currently encoding, at this point it outputs a “token”, which is &lt;16,6&gt; in this example, and continues. The &lt;16,6&gt; token is quite simple to understand too, it consists of two numbers and some syntactic sugar to make it easy to understand. The first number corresponds to how many characters it shoud look backwards, and the next number tells it how many characters to go forwards and copy. This means that in our example, &lt;16,6&gt; expands into “Hello “ as it goes 16 characters backwards, and copies the next 6 characters. This is the essential idea behind the algorithm, however it should be noted that there are many variations of this algorithm with different names. For example, in some implementations, the first number means go forwards from the beginning instead of backwards from the current position. Small (and big) differences like these are the reason for so many variations: . | LZSS - Lempel-Ziv-Storer-Szymanski | LZW - Lempel-Ziv-Welch | LZMA - Lempel–Ziv–Markov chain algorithm | LZ77 - Lempel-Ziv 77 | LZ78 - Lempel-Ziv 78 | . It’s also important to understand the difference between LZ77 and LZ78, the first, and most common, Lempel-Ziv algorithms. LZ77 works very similarly to the example above, using a token to represent an offset and length, while LZ78 uses a more complicated dictionary approach. For a more in-depth explanation, make sure to check out this wonderful article explaining LZ78. UNFINISHED . ",
    "url": "/algorithms/lz/#the-algorithms",
    "relUrl": "/algorithms/lz/#the-algorithms"
  },"19": {
    "doc": "Lempel-Ziv",
    "title": "Implementations",
    "content": "Now because there are so many different variations of Lempel-Ziv algorithms, there isn’t a single LZ implementation. WIth that being said, if you are interested in implementing a Lempel-Ziv algorithm yourself, you’ll have to choose an algorithm to start with. LZSS is a great starting point as it’s a basic evolution of LZ77 and can be implemented very easily while achieving a respectable compression ratio. If you’re interested in another algorithm, head back to the algorithms . ",
    "url": "/algorithms/lz/#implementations",
    "relUrl": "/algorithms/lz/#implementations"
  },"20": {
    "doc": "LZSS",
    "title": "Lempel-Ziv-Storer-Szymanski",
    "content": "Lempel-Ziv-Storer-Szymanski, which we’ll refer to as LZSS, is a simple variation of the common LZ77 algorithm. It uses the same token concept with an offset and length to tell the decoder where to copy the text, except it only places the token when the token is shorter than the text it is replacing. The idea behind this is that it will never increase the size of a file by adding tokens everywhere for repeated letters. You can imagine that LZ77 would easily increase the file size if it simply encoded every repeated letter “e” or “i” as a token, which may take at least 5 bytes depending on the file and implementation. ",
    "url": "/algorithms/lzss/#lempel-ziv-storer-szymanski",
    "relUrl": "/algorithms/lzss/#lempel-ziv-storer-szymanski"
  },"21": {
    "doc": "LZSS",
    "title": "Implementing an Encoder",
    "content": "Let’s take a look at some examples, so we can see exactly how it works. The wikipedia article for LZSS has a great example for this, which I’ll use here, and it’s worth a read as an introduction to LZSS. So let’s encode an exceprt of Dr. Seuss’s Green Eggs and Ham with LZSS (credit to Wikipedia for this example). I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! DO WOULD YOU LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM,SAM-I-AM. I DO NOT LIKE GREEN EGGS AND HAM. This text takes up 192 bytes in a typical UTF-8 encoding. Let’s take a look at the LZSS encoded version. I AM SAM. &lt;10,10&gt;SAM I AM. THAT SAM-I-AM! T&lt;15,14&gt;I DO NOT LIKE&lt;29,15&gt; DO WOULD YOU LIKE GREEN EGGS AND HAM? I&lt;69,15&gt;EM,&lt;113,8&gt;.&lt;29,15&gt;GR&lt;64,16&gt;. This encoded, or compressed, version only takes 148 bytes to store (without a magic type to describe the file type), which is a 77% of the original file size, or a compression ratio of 1.3. Not bad! . Analysis . Now let’s take a second understand what’s happening before you start trying to conquer the world with LZSS. As we can see, the “tokens” are reducing the size of the file by referencing pieces of text that are longer than the actual token. Let’s look at the first line: . I AM SAM. &lt;10,10&gt;SAM I AM. The encoder works character by character. On the first character, ‘I’, it checks it’s search buffer to see if it’s already seen an ‘I’. The search buffer is essentially the encoder’s memory, for every character it encodes, it adds it into the search buffer so it can “remember” it. Because it hasn’t seen an ‘I’ already (the search buffer is empty), it just outputs an ‘I’, adds it to the search buffer, and moves to the next character. The next character is ‘ ‘ (a space). The encoder checks the search buffer to see if it’s seen a space before, and it hasn’t so it outputs the space and moves forward. Once it gets to the second space (after “I AM”), the LZ77 starts to come into play. It’s already seen a space before because it’s in the search buffer so it’s ready to output a token, but first it tries to maximize how much text the token is referencing. If it didn’t do this you could imagine that for every character it’s already seen it would output something similar to &lt;5,1&gt;, which is 5 times larger than any character. So once it finds a character that it’s already seen, it moves on to the next character and checks if it’s already seen the next character directly after the previous chracter. Once it finds a sequence of characters that it hasn’t already seen, then it goes back one character to the sequence of characters it’s already seen and prepares the token. Once the token is ready, the difference between LZ77 and LZSS starts to shine. At this point LZ77 simply outputs the token, adds the characters to the search buffer and continues. LZSS does something a little smarter, it will check to see if the size of the outputted token is larger than the text it’s representing. If so, it will output the text it represents, not the token, add the text to the search buffer, and continue. If not, it will output the token, add the text it represents to the search buffer and continue. Coming back to our example, the space character has already been seen, but a space followed by an “S” hasn’t been seen yet (“ S”), so we prepare the token representing the space. The token in our case would be “&lt;3,1&gt;”, which means go back three characters and copy 1 character(s). Next we check to see if our token is longer than our text, and “&lt;3,1&gt;” is indeed longer than “ “, so it wouldn’t make sense to output the token, so we output the space, add it to our search buffer, and continue. This entire process continues until we get to the “I AM SAM. “. At this point we’ve already seen an “I AM SAM. “ but haven’t seen an “I AM SAM. S” so we know our token will represent “I AM SAM. “. Then we check to see if “I AM SAM. “ is longer than “&lt;10,10&gt;”, which it is, so we output the token, add the text to our search buffer and go along. This process continues, encoding tokens and adding text to the search buffer character by character until it’s finished encoding everything. Takeaways . There’s a lot of information to unpack here, but the algorithm at a high level is quite simple: . | Loop character by character | Check if it’s seen the character before . | If so, check the next character and prepare a token to be outputted . | If the token is longer than the text it’s representing, don’t output a token | Add the text to the search buffer and continue | . | If not, add the character to the search buffer and continue | . | . It’s important to remember that no matter the outcome, token or no token, the text is always appended to the search buffer so it can always “remember” the text it’s already seen. ",
    "url": "/algorithms/lzss/#implementing-an-encoder",
    "relUrl": "/algorithms/lzss/#implementing-an-encoder"
  },"22": {
    "doc": "LZSS",
    "title": "Implementation",
    "content": "Now let’s take a stab at building our very own version so we can understand it more deeply. As with most of these algorithms, we have an implementation written in Go in our raisin project. If you’re interested in what a more performant or real-world example of these algorithms looks like, be sure to check it out. However for this guide we’ll use Python to make it more approachable so we can focus on understanding the algorithm and not the nuances of the language. Character Loop . Let’s get started with a simple loop that goes over each character for encoding. As we can see from our takeaways, the character-by-character loop is what powers LZSS. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character . Output: . H E L L O . Although the code is functionally pretty simple, there’s a few important things going on here. You can see that looping character-by-character isn’t as simple as for char in text, first we have to encode it and then loop over the encoding. This is because it converts our string into an array of bytes, represented as a Python object called bytes. When we print the character out, we have to convert it from a byte (represented as a Python int) back to a string so we can see it. The reason we do this is because a byte is really just a number from 0-255 as it is represented in your computer as 8 1’s and 0’s, called binary. If you don’t already have a basic understanding of how computers store our language, you should get acquainted with it on our getting started page. Search Buffers . Great, we have a basic program working which can loop over our text and print it out, but that’s pretty far off from compression so let’s keep going. The next step to our program is to implement our “memory” so the program can check to see if its already seen a character. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . We no longer need to output anything as we’re just adding each character to the search buffer with the append method. Checking Our Search buffer . Now let’s try to implement the LZ part of LZSS, we need to start looking backwards for characters we’ve already seen. This can accomplished quite easily using the list.index method. for char in text_bytes: if char in search_buffer: print(f\"Found at {search_buffer.index(char)}\") print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . Output: . H E L Found at 2 L O . Notice the if char in search_buffer, without this Python will throw an IndexError if the value is not in the list. Building Tokens . Now let’s build a token and output it when we find the character. i = 0 for char in text_bytes: if char in search_buffer: index = search_buffer.index(char) # The index where the character appears in our search buffer offset = i - index # Calculate the relative offset length = 1 # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . H E L &lt;1,1&gt; O . We’re nearly there! This is actually a rough implementation of LZ77, however there’s one issue. If we have a word that repeats twice, it will copy each character instead of the entire word. text = \"SAM SAM\" . Output . S A M &lt;4,1&gt; &lt;4,1&gt; &lt;4,1&gt; . Note: &lt;4,1&gt; is technically correct as each character is represented 4 characters behind the beginning of the token. That’s not exactly right, we should see &lt;4,3&gt; instead of three &lt;4,1&gt; tokens. So let’s write some code that can check our search buffer for more than one character. Checking the Search Buffer for More Characters . Let’s modify our code to check the search buffer for more than one character. def elements_in_array(check_elements, elements): i = 0 offset = 0 for element in elements: if len(check_elements) &lt;= offset: # All of the elements in check_elements are in elements return i - len(check_elements) if check_elements[offset] == element: offset += 1 else: offset = 0 i += 1 return -1 text = \"SAM SAM\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M &lt;4,3&gt; . It works! But there’s quite a lot to unpack here so let’s go through it line by line. The first and largest addition is the elements_in_array function. The code here essentially checks to see if specific elements are within an array in an exact order. If so, it will return the index in the array where the elements start, and if not it will return -1. Moving on to our main function loop we can see now have check_characters defined at the top. This variable tracks what characters we’re looking for in our search_buffer. As we loop through, we use check_characters.append(char) to add the current character to the characters we’re searching. Then we check to see if check_characters can be found within search_buffer with elements_in_array. Now we have the best part: the logic. If we couldn’t find a match or it’s the last character we want to output something. If we couldn’t find more than one character in the search_buffer then that means check_characters minus the last character was found, so we’ll output a token representing check_characters minus the last character. Otherwise, we couldn’t find a match for a single character so let’s just output that character. And that’s essentially LZ77! Try it out for yourself with some different strings to see for yourself. However you might notice that we’re trying to implement LZSS, not LZ77, so we have one more piece to implement. Comparing Token Sizes . This crucial piece is the process described earlier of comparing the size of tokens versus the text it represents. Essnetially we’re saying, if the token takes up more space than the text it’s representing then don’t output a token, just output the text. Lucky for us this is a pretty simple change. Our main loop now looks like so: . for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character print(bytes(check_characters).decode(encoding)) # Print the characters else: print(token) # Print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M SAM . The key is the len(token) &gt; length which checks if the length of the token is longer than the length of the text it’s representing. If it is, it simply outputs the characters, otherwise it outputs the token. Sliding Windows . The last piece to the puzzle is something you might have noticed if you’re already trying to compress large file: the search buffer gets big. Let’s say we’re compressing a 1 Gb file. After we go over each character, we add it to the search buffer and continue, though each iteration we also search the entire search buffer for certain characters. This quickly adds up for larger files. In our 1 Gb file scenario, near the end we’ll have to search almost 1 billion bytes to encode a single character. It should be pretty obvious that this very inefficient. And unfortunately, there is no magic solution, you have to make a tradeoff. With every compression algorithm you have to decide between speed and compression ratio. Do you want a fast algorithm that can’t reduce the file size very much, or a slow algorithm that reduces the file size more? The answer is: it depends. And so, the tradeoff in LZ77’s case is to create a “sliding window”. The “sliding window” is actually quite simple, all you do is cap off the maximum size of the search buffer. When you add a character to the search buffer that makes it larger than the maximum size of the sliding window then you remove the first character. That way the window is “sliding” as you move through the file, and the algorithm doesn’t slow down! . max_sliding_window_size = 4096 ... for char in text_bytes: ... search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer ... These changes should be pretty self-explanatory. We’re just checking to see if the length of the search_buffer is greater than the max_sliding_window_size, and if so we pop the first element off of the search_buffer. Keep in mind that while a maximum sliding window size of 4096 character is typical, it may be hard to use during testing, try setting it much lower (like 3-4) and test it with some different strings to see how it works. Putting it all together . That’s everything that makes up LZSS, but for the sake of completing our example, let’s clean it up so we can call a function with some text, an optional max_sliding_window_size, and have it return the encoded text, rather than just printing it out. encoding = \"utf-8\" def encode(text, max_sliding_window_size=4096): text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes output = [] # Output array i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token else: output.extend(check_characters) # Output the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 return bytes(output) print(encode(\"SAM SAM\", 1).decode(encoding)) print(encode(\"supercalifragilisticexpialidocious supercalifragilisticexpialidocious\", 1024).decode(encoding)) print(encode(\"LZSS will take over the world!\", 256).decode(encoding)) print(encode(\"It even works with 😀s thanks to UTF-8\", 16).decode(encoding)) . The function definition is pretty simple, we can just move our text and max_sliding_window_size outside of the function and wrap our code in a function definition. Then we simply call it with some different values to test it, and that’s it! . The finished code can be found in lzss.py in the examples GitHub repo. Lastly, there’s a few bug in our program that we encounter with larger files. If we have some text, for example: . ISAM YAM SAM . When the encoder gets to the space right before the “SAM”, it will look for a space in the search buffer which it finds. Then it will search for a space and an “S” (“ S”) which it doesn’t find, so it continues and starts looking for an “A”. The issue here is that it skips looking for an “S” and continues to encode the “AM” not the “SAM”. In some rare circumstances the code may generate a reference that with a length that is larger than its offset which will result in an error. To fix this, we’ll need to rewrite the logic in our encoder a little bit. for char in text_bytes: index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if elements_in_array(check_characters + [char], search_buffer) == -1 or i == len(text_bytes) - 1: if i == len(text_bytes) - 1 and elements_in_array(check_characters + [char], search_buffer) != -1: # Only if it's the last character then add the next character to the text the token is representing check_characters.append(char) if len(check_characters) &gt; 1: index = elements_in_array(check_characters, search_buffer) offset = i - index - len(check_characters) # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the characters output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token search_buffer.extend(check_characters) # Add the characters to our search buffer else: output.extend(check_characters) # Output the character search_buffer.extend(check_characters) # Add the characters to our search buffer check_characters = [] check_characters.append(char) if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 . To fix the first issue we add the current char to check_characters only at the end and check to see if check_characters + [char] is found. If not we know that check_characters is found so we can continue as normal, and check_characters gets cleared before char is added onto check_characters for the next iteration. We also implement a check on the last iteration to add the current char to check_characters as otherwise our logic wouldn’t be run on the last character and a token wouldn’t be created. To resolve the other problem we simply have to move the search_buffer.append(char) calls up into our logic and change them to search_buffer.extend(check_characters). This way we only update our search buffer when we’ve already tried to find a token. ",
    "url": "/algorithms/lzss/#implementation",
    "relUrl": "/algorithms/lzss/#implementation"
  },"23": {
    "doc": "LZSS",
    "title": "Implementing a Decoder",
    "content": "What’s the use of encoding something some text if we can’t decode it? For that we’ll need to build ourselves a decoder. Luckily for us, building a decoder is actually much simpler than an encoder because all it needs to know how to do is convert a token (“&lt;5,2&gt;”) into the literal text it represents. The decoder doesn’t care about search buffers, sliding windows, or token lengths, it only has one job. So, let’s get started. We’re going to decode character-by-character just like our encoder so we’ll start with our main loop inside of a function. We’ll also need to encode and decode the strings so we’ll keep the encoding = \"utf-8\". encoding = \"utf-8\" def decode(text): text_bytes = text.encode(encoding) # The text encoded as bytes output = [] # The output characters for char in text_bytes: output.append(char) # Add the character to our output return bytes(output) print(decode(\"supercalifragilisticexpialidocious &lt;35,34&gt;\").decode(encoding)) . Here we’re setting up the structure for the rest of our decoder by setting up our main loop and declaring everything within a neat self-contained function. Identifying Tokens . The next step is to start doing some actual decoding. The goal of our decoder is to convert a token into text, so we need to first identify a token and extract our offset and length before we can convert it into text. Notice the various components of a token that need to be identified and extracted so we can find the text they represent . Let’s make a small change so we can identify the start and end of a token. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: print(\"Found opening of a token\") elif char == \"&gt;\".encode(encoding)[0]: print(\"Found closing of a token\") output.append(char) # Add the character to our output return bytes(output) . Because we’re going character-by-character we can simply check to see if the character is a token opening character or closing character to tell if we’re inside a token. Let’s add some more code to track the numbers between the comma, our seperator. inside_token = False scanning_offset = True length = [] # Length number encoded as bytes offset = [] # Offset number encoded as bytes for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) print(f\"Found token with length: {length_num}, offset: {offset_num}\") # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) output.append(char) # Add the character to our output . Output: . Found token with length: 34, offset: 35 supercalifragilisticexpialidocious &lt;35,34&gt; . We now have a bunch of if statements that give our loop some more control flow. Let’s go over the changes. First off we have four new variables outside of the loop: . | inside_token - Tracks whether or not we’re inside a token | scanning_offset - Tracks whether we’re currently scanning for the offset number or the length number (1st or 2nd number in the token) | length - Used to store the bytes (or characters) that represent the token’s length | offset- Used to store the bytes (or characters) that represent the token’s offset | . Inside of the loop, we check if the character is a &lt;, ,, or a &gt; and modify the variables accordingly to track where we are. If the character isn’t any of those and we’re inside a token then we want to add the character to either the offset or length because that means the character is an offset or length. Lastly, if the character is a &gt;, that means we’re exiting the token, so let’s convert our length and offset into a Python int. We have to do this because they’re currently represented as a list of bytes, so we need to convert those bytes into a Python string and convert that string into an int. Then we finally print that we’ve found a token. Translating Tokens . Now we have one last step left: translating tokens into the text they represent. Thanks to Python list slicing this is quite simple. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number token_start = i elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) # Get text that the token represents referenced_text = output[-offset_num:][:length_num] output.extend(referenced_text) # referenced_text is a list of bytes so we use extend to add each one to output # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) else: output.append(char) # Add the character to our output return bytes(output) . Output: . supercalifragilisticexpialidocious supercalifragilisticexpialidocious . In order to calculate the piece of text that a token is referencing we can simply use our offset and length to find the text from the current output. We use a negative slice to grab all the characters backwards from offset_num and grab up to length_num elements. This results in a referenced_text that represents the token references. Finally we add the referenced_text to our output and we’re finished. Lastly, we’ll only want to add a character to the output if we’re not in a token so we add an else to the end of our logic which only runs if we’re not in a token. And that’s it! We now have a LZSS decoder, and by extension, an LZ77 decoder as decoders don’t need to worry about outputting a token only if it’s greater than the referenced text. ",
    "url": "/algorithms/lzss/#implementing-a-decoder",
    "relUrl": "/algorithms/lzss/#implementing-a-decoder"
  },"24": {
    "doc": "LZSS",
    "title": "Implementation Conclusion",
    "content": "We’ve gone through step-by-step building an encoder and decoder and learned the purpose of each component. Let’s do some basic benchmarks to see how well it works. if __name__ == \"__main__\": encoded = encode(text).decode(encoding) decoded = decode(encoded).decode(encoding) print(f\"Original: {len(text)}, Encoded: {len(encoded)}, Decoded: {len(decoded)}\") print(f\"Lossless: {text == decoded}\") print(f\"Compressed size: {(len(encoded)/len(text)) * 100}%\") . Using the text as Green Eggs and Ham by Doctor Seuss, we see the output: . Original: 3463 bytes, Encoded: 1912 bytes, Decoded: 3463 bytes Lossless: True Compressed size: 55.21224371931851% . LZSS just reduced the file size by 45%, not bad! . One thing to keep in mind is that when we refer to a “character”, we really mean a “byte”. Our loop runs byte-by-byte, not character-by-character. This distinction is minor but significant. In the world of encodings, not every character is a single byte. For example in utf-8, any english letter or symbol is a single byte, but more complicated characters like arabic, mandarin, or emoji characters require multiple bytes despite being a single “character”. | 4 bytes - 😀 | 1 byte - H | 3 bytes - 话 | 6 bytes - يَّ | . If you’re interested in learning more about how bytes work, check out the Wikipedia articles on Bytes and Unicode. ",
    "url": "/algorithms/lzss/#implementation-conclusion",
    "relUrl": "/algorithms/lzss/#implementation-conclusion"
  },"25": {
    "doc": "LZSS",
    "title": "LZSS",
    "content": " ",
    "url": "/algorithms/lzss/",
    "relUrl": "/algorithms/lzss/"
  },"26": {
    "doc": "Unix Magic Types",
    "title": "Unix Magic Types",
    "content": "Stub. ",
    "url": "/reference/magic_types/",
    "relUrl": "/reference/magic_types/"
  },"27": {
    "doc": "Overview of Algorithms",
    "title": "Overview of Algorithms",
    "content": "The following is intended to be a comprehensive list of lossless compression algorithms (in no particular order), however if you feel like an algorithm is missing, please let us know. | Run-length Coding | Range Coding | Lempel-Ziv . | LZ77 | LZ78 | LZSS | LZW | . | Variable-length Coding | Huffman Coding | Arithmetic Coding | Dynamic Markov Compression | FLATE | . For a more complete list, check out these Wikipedia pages on lossless algorithms and lossy algorithms. ",
    "url": "/algorithms/overview/",
    "relUrl": "/algorithms/overview/"
  },"28": {
    "doc": "Reference",
    "title": "Reference",
    "content": "This section serves to be a reference to topics that aren’t directly related to compression, but inveitably come into play. A basic understanding of these concepts are invaluable when building your own implementation or algorithm. ",
    "url": "/reference/reference/",
    "relUrl": "/reference/reference/"
  }
}
