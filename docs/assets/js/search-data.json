{"0": {
    "doc": "Arithmetic Encoder",
    "title": "Arithmetic Encoder",
    "content": " ",
    "url": "/interactive/arithmetic/",
    "relUrl": "/interactive/arithmetic/"
  },"1": {
    "doc": "Arithmetic Coding",
    "title": "Arithmetic Coding",
    "content": "In the world of dictionary coding and probability based encoding, the floating point weirdness that is arithmetic coding is a refreshing and surprisingly efficient lossless compression algorithm. The algorithm takes the form of two stages, the first stage translates a string into a floating point range and the second stage translates this into a binary sequence. Let’s take a look at how each stage works. ",
    "url": "/algorithms/arithmetic/",
    "relUrl": "/algorithms/arithmetic/"
  },"2": {
    "doc": "Arithmetic Coding",
    "title": "Stage 1: Floating Point Ranges",
    "content": "At a very broad level arithmetic coding works by taking a character and assigning it a frequency to a table. This frequency is then mapped to a number line between 0 and 1. So, if we have the character frequency table as shown below for the word “HELLO”, we would end up with our number line shown below. | Character | Frequency | Probability | . | H | 1 | 20% | . | E | 1 | 20% | . | L | 2 | 40% | . | O | 1 | 20% | . To do the encoding, we need a floating point range representing our encoded string. So, for example, let’s encode “HELLO”. We start out by encoding just the letter “H”, which would give us the range of 0 to 0.2. However, we’re not just encoding “H” so, we need to encode “E”. To encode “E” we take the range from encoding “H”, 0 to 0.2, and apply our same frequency table to that. You can see this represented below. Blown up, you can see that we’re essentially copying the number line down, but fitting it within the range of 0 to 0.2 instead of 0 to 1. Now, we’ll encode the letter “E”, and we can see it falls within the range of 0.04 to 0.08. As we move through this process, this copying of the number line and fitting it within the previous range continues until we encode our entire string. Though, if you’re familiar with floating point arithmetic in computers, you know that computers aren’t good with decimals, especially long ones. There are some workarounds to this, but generally floating point math is too inefficient or innaccurate to make arithmetic coding work quickly or properly for compression. The answer to this issue is called finite-precision arithmetic coding, with the above approach of fitting the number line within a range known as the infinite-precision version because we (supposedly) have an infinite amount of precision. Now if we continue this process, we get a range representing 0.06368 to 0.06496. The difference between these two numbers is just 0.00128, a big difference from the 0.2 difference when encoding just “H”. You can imagine that larger files will have an even smaller difference between the two ranges, spelling out the need for finite-precision arithmetic coding. ",
    "url": "/algorithms/arithmetic/#stage-1-floating-point-ranges",
    "relUrl": "/algorithms/arithmetic/#stage-1-floating-point-ranges"
  },"3": {
    "doc": "Arithmetic Coding",
    "title": "Stage 2: Binary Search",
    "content": "The next, and luckily final, stage is to run a binary search-like algorithm over the table to find a binary range that lays within our range from the first stage. The way this works is actually quite simple. We take our number line from 0 to 1, and lay it out. Then, we plot our range on the number line, and place our current target in the middle of the range: 0.5. The trick is to see whether our range falls on the left or right side of our target (0.5). In this case, our range pretty clearly falls on the left hand side so we output a 0. If it fell on the right hand side we would output a 1. Now here’s where things get interesting. We change the top end of our range from 1 to 0.5, so now we’re looking at the range from 0 to 0.5, with out target at 0.25 (0.25 is in between 0 and 0.5). You can see the range moves closer to our target and the area between gets a little larger. It’s important to note we’re not changing the range, just looking at it magnified. Our range is still below 0.25 so we’ll output a 0 and repeat this process for the range of 0-0.25. This continues until you’re left with a binary sequence that represents a target, just like the 0.5 and 0.25 from earlier examples, that lays within our encoded range from stage 1. This binary stream is the coded version (the compressed version) as we can use it to get back to the original string with the right frequency table. ",
    "url": "/algorithms/arithmetic/#stage-2-binary-search",
    "relUrl": "/algorithms/arithmetic/#stage-2-binary-search"
  },"4": {
    "doc": "Arithmetic Coding",
    "title": "Infinite vs. Finite Precision",
    "content": "Infinite precision is the process that we just went over with two stages. However as we saw, the more characters we encode, the smaller the difference between our range floor and ceiling gets. This means that as we encode more and more characters the top and bottom sections of the range will eventually meet and represent the same value because a typical 32-bit system cannot represent infinite precision. There are ways around this, such as increasing the size of the floating point number’s precision or using infinite precision, but these solutions don’t work for all data or are very inefficient respectively. The harder solution is to combine these steps in one stage, which is called finite-precision arithmetic coding because it only requires a finite amount of precision to operate. This version works by encoding the first character, then immediately trying to see if the range falls above or below 0.5. If so, it will output the binary number representing which half it lays within and will “blow up” the range so that it doesn’t lose precision. There is also an important corner-case of encoding a “10” or “01” if the range lays within 0.25-0.75 which requires memory to be carried over from each encoding. To put it simply, infinite-precision arithmetic coding is a simple and easy way to understand arithmetic coding while finite-precision arithmetic coding is more complicated but scalable and efficient. Now unfortunately I can’t explain how to implement your own version of finite-precision arithmetic coding well enough to be comprehensive, so I’ll redirect you to a wonderful article by Mark Nelson that explains how to write an arithmetic coder with infinite and finite precision. There are also some wonderful online lectures by mathematicalmonk on YouTube that go into detail about finite-precision coding in a visual way. If anything in this article doesn’t make sense to you then I can’t recommend mathematicalmonk’s YouTube lectures and Mark Nelson’s article. Arithmetic Compression from Compressor Head on YouTube is also a great and enjoyable primer on the topic. ",
    "url": "/algorithms/arithmetic/#infinite-vs-finite-precision",
    "relUrl": "/algorithms/arithmetic/#infinite-vs-finite-precision"
  },"5": {
    "doc": "Arithmetic Coding",
    "title": "Adaptive Arithmetic Coding",
    "content": "One last variation of arithmetic coding worth mentioning is the idea of adaptive arithmetic coding. This version works in mostly the same way as typical arithmetic coding except that rather than building a frequency table from the source, it builds the frequency table as it encodes each character. This means if you were encoding “HELLO”, it would start with “H” representing 100% of the table. When it encoded the “E”, it would update the frequency table so “H” would represent 50% of the table and “E” representing the remaining 50%. This concept allows arithmetic coding to adapt to the content as it’s encoding which allows it to achieve a higher compression ratio. ",
    "url": "/algorithms/arithmetic/#adaptive-arithmetic-coding",
    "relUrl": "/algorithms/arithmetic/#adaptive-arithmetic-coding"
  },"6": {
    "doc": "Arithmetic Coding",
    "title": "Resources",
    "content": "If you’re interested in learning more about arithmetic coding, check out these great resources: . | Mark Nelson - Data Compression With Arithmetic Coding | Arithmetic Compression from Compressor Head | mathematicalmonk - Arithmetic Coding | . ",
    "url": "/algorithms/arithmetic/#resources",
    "relUrl": "/algorithms/arithmetic/#resources"
  },"7": {
    "doc": "Attributions",
    "title": "Attributions",
    "content": "This non-exhaustive list of wonderful projects, frameworks, tools, and libraries were used to help build this website. | Just the Docs Jekyll theme | Jekyll | pretty-checkbox.css | noUiSlider | Pure CSS | Fabric.js | . ",
    "url": "/attributions/",
    "relUrl": "/attributions/"
  },"8": {
    "doc": "Bytes and Binary",
    "title": "Bytes and Binary",
    "content": "When it comes to working with computers, one thing becomes very apparently: there’s a lot of data. Now naturally, when you have a lot of data you have to figure out how you want to store the data. Just like how leftover Christmas decorations are stored in bins inside of garages, data is stored inside of bytes inside of a file. So what is a byte? . Bytes are a term used to represent 8 bits, simply a 1 or a 0. So, a byte might look something like this: 00111010. ",
    "url": "/reference/bytes/",
    "relUrl": "/reference/bytes/"
  },"9": {
    "doc": "Bytes and Binary",
    "title": "Binary",
    "content": "These ones and zeroes are used in something called binary. Now, in most of the world you count with a base-10 decimal number system. One is 1, two is 2, all the way up to 10 being 10, and so forth. The number system is based around 10 being the base. | Base | Power | Decimal | . | 10 | 0 | 1 | . | 10 | 1 | 10 | . | 10 | 2 | 100 | . | 10 | 3 | 1,000 | . Now binary works the same way, except the base is actually 2, not 10. | Base | Power | Decimal | Binary | . | 2 | 0 | 1 | 0001 | . | 2 | 1 | 2 | 0010 | . | 2 | 2 | 4 | 0100 | . | 2 | 3 | 8 | 1000 | . With this simple example, you start to see a simple pattern emerge. The right-most bit (the 1 or 0) represents 2^0, left of that is 2^1, left of that is 2^2, etc. Using this simple system we can represent any number using 1s and 0s, which is precisely what a computer uses to store numbers. Let’s take a look at a few more examples because unless you have a PhD in mathematics or computer science, the entire “different number base” thing might be still be a bit confusing. So let’s take the binary number 0110010, and convert it to decimal so our human minds can understand what it means. Now remember, the right most bit (also known as the least significant bit) represents 2^0, which is 1. This means that if the right most bit is a 1, then we add 1 to the result, otherwise we add 0. The next bit to the left represents 2^1, which is 2, so if it’s a 1, then we add a 2, otherwise we add a 0. Hopefully the pattern is starting to become clear: . | Binary | 0 | 1 | 1 | 0 | 0 | 1 | 0 | . | Decimal | 64 | 32 | 16 | 8 | 4 | 2 | 1 | . Then we can add our decimal numbers together: . | 0*64 | 1*32 | 1*16 | 0*8 | 0*4 | 1*2 | 0*1 |   | . | 0 + | 32 + | 16 + | 0 + | 0 + | 2 + | 0 | = 50 | . And we get 50! . If it’s still a bit cloudy or you would just like some more practice, make sure to search online for information about “binary numbers”, or check out some of these great resources: . | Wikipedia | RapidTables Binary Calculator | Visual 8-bit Binary to Decimal Converter | . So now that you understand binary, where do they fit within bytes and how do they store text? . ",
    "url": "/reference/bytes/#binary",
    "relUrl": "/reference/bytes/#binary"
  },"10": {
    "doc": "Bytes and Binary",
    "title": "Bytes",
    "content": "Bytes are the answer to storing text, and basically everything, on a computer. A byte is essentially a chunk of bits (1s and 0s), generally 8. Bytes are used nearly everywhere in a computer, they’re used to store variables in memory, to transfer data over wires or the internet, and most importantly to compression, to store data in files. Every character in English is stored as a byte in a file. And as we know a byte is 8 bits. So, from our knowledge we know that the largest number an 8 bit binary number can store is 2^8, or 256. This means that a single byte can store exactly 256 different numbers, or in our case, 256 unique characters. In 1960 computers are still brand new and largely used by English speaking countries, so there was a need for a system to represent English characters, without worrying about any other language. The answer to this is ASCII. While there’s a massive amount of history behind ASCII and all of the other computer history being created at the time, the basis is simple. The English alphabet, and generally every other special character English uses can be assigned a number from 0-255 (remember the largest 8 bit number is 11111111, which is only 255 in decimal, but 00000000 represents 0, so there are 256 possibilities). Let’s take a look at the ASCII codes for capital A-F on the alphabet: . | Letter | ASCII Code | Binary Code | . | A | 65 | 01000001 | . | B | 66 | 01000010 | . | C | 67 | 01000011 | . | D | 68 | 01000100 | . | E | 69 | 01000101 | . | F | 70 | 01000110 | . For a complete table, check out the RapidTables ASCII Table. These binary representations of each character would then be stored in a file. Later when you wanted to read the file again, your computer would take each chunk of 8 bits, lookup the letter it represents in an ASCII table, and output the represented character before moving on. This system worked perfectly for a while until more people started using computers. Now as it turns out, there are a lot of people who want to use computers who don’t speak English and need to represent characters that we don’t have in English (Arabic, Spanish, French, Chinese, etc.). The solution to this is using a different type of encoding rather than ASCII, referred to as file encodings. However, the most common encoding, UTF-8, simply extends ASCII to add extra characters because ASCII does not use all 256 numbers, it only uses around 127. So, in a modern day file, the word “Hello” would be represents as follows: . Text: Hello UTF-8 Decimal: 72 101 108 108 111 UTF-8 Binary: 01001000 01100101 01101100 01101100 01101111 . To try this out on your own, check out Online Utf8 Tools. ",
    "url": "/reference/bytes/#bytes",
    "relUrl": "/reference/bytes/#bytes"
  },"11": {
    "doc": "Bytes and Binary",
    "title": "Endianness",
    "content": "When working with bytes and storing them in memory you run into a problem. How do you store a sequency of bytes in memory? So, for example, let’s say we have the following bytes: . | 11110000 | 01011010 | . And let’s save we have some memory allocated for our 2 bytes that looks like this: . | (empty byte) | (empty byte) | . How do you want to store the bytes in memory? . Looking at it like this, the simple solution is to just store them sequentially with the first byte being stored in the first byte of memory, and the second byte in the second byte of memory. Our memory would then look like this: . | 11110000 | 01011010 | . This approach is called big endian ordering. The other approach is called little endian ordering which, you guessed it, is the opposite. If we were to store the same two bytes with little endian ordering in memory, it would look like this: . | 01011010 | 11110000 | . Now using big vs little endian ordering doesn’t make a large difference in practice, it’s just important that all of the software and hardware interacting with the memory knows which endian ordering it’s using. It’s also important to ensure that other computers receiving data know what endian ordering the data is being transmitted with so the other computer can correctly interpret the bytes. If endianness still doesn’t make complete sense or you’re interested in learning more, you should check out Computerphile’s video about endianness or the article about it on Wikipedia. ",
    "url": "/reference/bytes/#endianness",
    "relUrl": "/reference/bytes/#endianness"
  },"12": {
    "doc": "Bytes and Binary",
    "title": "Kilobytes, Megabytes, and Beyond",
    "content": "When dealing with data in the real world, you’ll often encounter files larger than a few hundreds bytes. Often these files can reach into the thousands, millions, and even billions of bytes. Luckily, rather than saying that a file of 1,000,000 bytes is 1,000,000 bytes, we can simply say it is 1 megabyte. This is the same reason we say the Moon is 384,400 km away from the Earth, not 384,400,000 meters. Bytes use standard metric prefixes, so any metric prefix means the same thing when talking about meters, bytes, or any other metric measurement. | Name | Base 10 | Decimal | . | Kilobyte | $ 10 ^ 3 $ | 1,000 | . | Megabyte | $ 10 ^ 6 $ | 1,000,000 | . | Gigabyte | $ 10 ^ 9 $ | 1,000,000,000 | . | Terabyte | $ 10 ^ {12} $ | 1,000,000,000,000 | . | Petabyte | $ 10 ^ {15} $ | 1,000,000,000,000,000 | . You can learn more about binary prefixes on Wikipedia. ",
    "url": "/reference/bytes/#kilobytes-megabytes-and-beyond",
    "relUrl": "/reference/bytes/#kilobytes-megabytes-and-beyond"
  },"13": {
    "doc": "Bytes and Binary",
    "title": "Resources",
    "content": "If you want to learn more about bytes and binary in general, check out some of these great resources: . | Binary on Wikipedia | Endianness on Wikipedia | Byte on Wikipedia | Bit on Wikipedia | Khan Academy Binary Course | Byte Size Infographic | Why Use Binary? - Computerphile | Endianness Explained With an Egg - Computerphile | . ",
    "url": "/reference/bytes/#resources",
    "relUrl": "/reference/bytes/#resources"
  },"14": {
    "doc": "Compression Ratios",
    "title": "Compression Ratios",
    "content": "Compression ratios are generally used to represent how good a compression algorithm is at compressing. Generally, this is represented as the uncompressed size divided by the compressed size, yielding a number (hopefully) greater than 1. The higher the compression ratio, the better the compression algorithm is. Equation from Wikipedia . It should also be noted that a better compression ratio does not always indicate a better compression algorithm. Some algorithms are designed to give a moderate compression ratio with very good speed, while others are focused on good compression ratios and moderate speed. The use case of a compression algorithm are what determines what factors of a compression algorithm are favorable. For example, when streaming video you must be able to decode each frame relatively quickly, but when downloading a large game it may be preferable to download a smaller file and take time to decode the compressed files. ",
    "url": "/reference/compression_ratios/",
    "relUrl": "/reference/compression_ratios/"
  },"15": {
    "doc": "Compression Ratios",
    "title": "Resources",
    "content": "Some good resources to learn more about compression algorithms include: . | Data compression ratio on Wikipedia | Comparison of Algorithms (helpful to see how compression ratios are compared) | Comparison of Different Compression Algorithms (helpful to see how compression ratios are compared) | . ",
    "url": "/reference/compression_ratios/#resources",
    "relUrl": "/reference/compression_ratios/#resources"
  },"16": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": "This site is intended to be used as living documentation that is constantly updated by the community, if you feel like something’s missing or would simply like to contribute, please create an issue or create a pull request. The best way to ask a question is to file an issue and we’ll try to get back to you as soon as possible. ",
    "url": "/contributing/",
    "relUrl": "/contributing/"
  },"17": {
    "doc": "Dictionary Coding",
    "title": "Dictionary Coding",
    "content": "Dictionary coding one of the most primitive and powerful forms of compression that exists currently. In fact, we use it everyday in English. I’ve actually used it already. Did you catch that? . The contraction “I’ve” is technically a form of dictionary coding because when we read the word we automatically expand it to “I have” in our mind. This simple concept is used everywhere from spoken languages, mathematic functions, to file encodings. More advanced forms of dictionary coding form the basis for many different compression algorithms aside from a simple search-and-replace step such as LZ, Huffman, and more. ",
    "url": "/algorithms/dictionary/",
    "relUrl": "/algorithms/dictionary/"
  },"18": {
    "doc": "Dictionary Coding",
    "title": "The Concept",
    "content": "Let’s get an idea of how you could use dictionary coding to compress data. So let’s say we’re working in a restaurant and we have to communicate to the chefs what food we need to be prepared on paper. Now our restaurant has three things on the menu: . | Pizza | Fries | Milkshakes | . Rather than writing down “pizza” or “fries” everytime someone order’s pizza or fries, we can assign each item a unique code. | 1 - Pizza | 2 - Fries | 3 - Milkshake | . Now when we’re preparing order tickets for the kitchen, we can simply write 1, 2, or 3. This simple concept is used to improve upon the most advanced modern compression algorithms. ",
    "url": "/algorithms/dictionary/#the-concept",
    "relUrl": "/algorithms/dictionary/#the-concept"
  },"19": {
    "doc": "Dictionary Coding",
    "title": "Implementation",
    "content": "Implementing a dictionary coder and decoder is actually very simple. All we’re really doing is replacing the long text with corresponding codes to encode it, and replacing codes with the text it represents to decode it. Here’s a sample: . text = \"Order: pizza, fries, milkshake\" encoded = text.replace(\"pizza\", \"1\").replace(\"fries\", \"2\").replace(\"milkshake\", \"3\") print(encoded) # Order: 1, 2, 3 . Decoding is the same, just in reverse: . text = \"Order: 1, 2, 3\" decoded = text.replace(\"1\", \"pizza\").replace(\"2\", \"fries\").replace(\"3\", \"milkshake\") print(decoded) # Order: pizza, fries, milkshake . If we want to get a bit fancier we can even use a dictionary to dynamically pull codes and values from: . codes = {\"pizza\": \"1\", \"fries\": \"2\", \"milkshake\": \"3\"} text = \"Order: pizza, fries, milkshake\" encoded = text for value, code in codes.items(): encoded = encoded.replace(value, code) print(encoded) # Order: 1, 2, 3 . And to decode: . codes = {\"pizza\": \"1\", \"fries\": \"2\", \"milkshake\": \"3\"} text = \"Order: 1, 2, 3\" decoded = text for value, code in codes.items(): decoded = decoded.replace(code, value) print(decoded) # Order: pizza, fries, milkshake . ",
    "url": "/algorithms/dictionary/#implementation",
    "relUrl": "/algorithms/dictionary/#implementation"
  },"20": {
    "doc": "Dictionary Coding",
    "title": "Caveats",
    "content": "The key to dictionary coding is that it solves a different problem than general-purpose encoders. A dictionary coder must be built with a known list of words (or bytes) that are very common to be able to see any real difference. For example, you could build a dictionary coder with the most common English character and encode Shakespeare, which would probably give you a good compression ratio. But if you were to use the same dictionary to encode a PDF or HTML file you would get much worse results. You also have to make sure that both the coder and decoder have the same dictionaries, otherwise the encoded text will be useless. ",
    "url": "/algorithms/dictionary/#caveats",
    "relUrl": "/algorithms/dictionary/#caveats"
  },"21": {
    "doc": "Dictionary Coding",
    "title": "Common Usage",
    "content": "While dictionary coders may sound imperfect and niche, they’re quite the opposite. Web compression algorithms like Brotli use a dictionary with the most common words, HTML tags, JavaScript tokens, and CSS properties to encode web assets. This dictionary, while large, is insignificant compared to the savings they provide to each file they decode. Here’s some modern algorithms that employ dictionary coding: . | Brotli | Zstandard | . ",
    "url": "/algorithms/dictionary/#common-usage",
    "relUrl": "/algorithms/dictionary/#common-usage"
  },"22": {
    "doc": "Dictionary Coding",
    "title": "Resources",
    "content": "If you’re interested in learning more about dictionary coding, check out some of these great resources: . | Dictionary coder on Wikipedia | University of Washington Dictionary Coding | . ",
    "url": "/algorithms/dictionary/#resources",
    "relUrl": "/algorithms/dictionary/#resources"
  },"23": {
    "doc": "Dynamic Markov Compression",
    "title": "Dynamic Markov Compression",
    "content": "Dynamic Markov Compression is an obscure form of compression that uses Markov chains to model the patterns represented in a file. ",
    "url": "/algorithms/dmc/",
    "relUrl": "/algorithms/dmc/"
  },"24": {
    "doc": "Dynamic Markov Compression",
    "title": "Markov Chains",
    "content": "Markov Chains are a simple way to model the transitions between states based on a measureable probability. For example, we could use a Markov Chain to model the weather and the probability that it will become sunny if it’s already raining, or vice-versa. Each circle represents a state, and each arrow represents a transition. In this example, we have two states, raining and sunny, a perfect representation of true weather. Each state has two possible transitions, it can transition to itself again or it can transition to another state. The likelihood of each transition is defined by a percentage representing the probability that the transition occurs. Now let’s say it’s sunny and we’re following this model. According to the model there’s a 50% chance it’s sunny again tomorrow or a 50% chance it’s rainy tomorrow. If it becomes rainy, then there’s a 25% chance it’s rainy the day after that or a 75% chance it’s sunny the day after that. Markov Chains may sound scary but the essence of how they work is quite simple. Markov Chains are the statistical model behind a lot of the technology we use today from Google’s PageRank search algorithm to predictive text on smartphone keyboards. If you’d like to learn more, check out this wonderful article by Victor Powell and Lewis Lehe that goes into depth about how Markov Chains work. They also have a wonderful interactive demo. ",
    "url": "/algorithms/dmc/#markov-chains",
    "relUrl": "/algorithms/dmc/#markov-chains"
  },"25": {
    "doc": "Dynamic Markov Compression",
    "title": "Markov Chain Powered Compression",
    "content": "Dynamic Markov Compression is a very obscure and complicated subject when it comes to implementation, and unfortunately I cannot claim to understand it well enough to explain it myself. Though, I have written a similar algorithm from my own trial-and-error that employs stateful Markov chains that model a file. You can find the source code within the Raisin project under the compressor/mcc package. This code is un-optimized and a bit messy as it exists only as a research project to learn more about DMC. If you have a better understanding of DMC and would like to contribute to this article, we would appreciate any and all contributions! . ",
    "url": "/algorithms/dmc/#markov-chain-powered-compression",
    "relUrl": "/algorithms/dmc/#markov-chain-powered-compression"
  },"26": {
    "doc": "Dynamic Markov Compression",
    "title": "Resources",
    "content": "If you’re interested in trying to implement DMC yourself or are just interested in the algorithm, here’s a few helpful resources as a jumping-off point: . | Dynamic Markov Compression on Wikipedia | An Exploration of Dynamic Markov Compression Thesis | The structure of DMC (Paywall) | Original Paper | Original C Implementation by Gordon Cormack | Markov Chain Compression - Compressor Head | Markov Chains | . ",
    "url": "/algorithms/dmc/#resources",
    "relUrl": "/algorithms/dmc/#resources"
  },"27": {
    "doc": "Character Encodings",
    "title": "Character Encodings",
    "content": "When it comes to encoding every character that we use as a number there are many different ways we can do it. For example, how many bytes should a character represent? Should byte length be dynamic based on the character? Should you prioritize shorter encodings for more frequent characters? . These are the types of questions that are answered with the many different character encodings that exist today. The most popular of these are arguably UTF-8 and UTF-16, both encodings for Unicode. ",
    "url": "/reference/encodings/",
    "relUrl": "/reference/encodings/"
  },"28": {
    "doc": "Character Encodings",
    "title": "UTF-8 and UTF-16",
    "content": "UTF-8 is covered briefly in the the reference on bytes and binary, but generally the first 127 numbers in UTF-8 are the same as the ASCII encoding which makes transitioning between the two formats very easy. Additionally because of this property, UTF-8 results in smaller file sizes for primarily English data which is another contributing factor in it’s popularity. In order for UTF-8 to replicate ASCII encodings, it must use anywhere from one to four bytes to represent all 1,112,064 different characters defined by Unicode. This means that a single rendered character could take up 4 bytes which can be inefficient depending on what data you are encoding. The largest alternative encoding is UTF-16 which is very similar to UTF-8 except that it uses one or two 16-bit units, essentially two bytes or four bytes, never one or three. The advantage of doing this is that in some cases where a large range of non-English characters are being encoded, such as Mandarin or emoji characters, it can be more efficient than UTF-8 encodings. ",
    "url": "/reference/encodings/#utf-8-and-utf-16",
    "relUrl": "/reference/encodings/#utf-8-and-utf-16"
  },"29": {
    "doc": "Character Encodings",
    "title": "Other Encodings",
    "content": "Aside from UTF-8 and UTF-16 many different types of file encodings exist such as UTF-32, ISO standardized encodings, Mac OS Roman, Windows encodings, and more. Luckily however, Unicode has risen in popularity to the point that there is generally great support and standardization around Unicode encodings like UTF-8 and UTF-16. If you are interested in checking out some other file encodings, you should check out Wikipedia’s list on common character encodings. ",
    "url": "/reference/encodings/#other-encodings",
    "relUrl": "/reference/encodings/#other-encodings"
  },"30": {
    "doc": "Character Encodings",
    "title": "Resources",
    "content": "If you’re interested in learning more about Unicode and other character encodings, check out some of these great resources: . | Character encoding on Wikipedia | Unicode on Wikipedia | Difference between UTF-8, UTF-16 and UTF-32 Character Encoding | Comparison of Unicode encodings on Wikipedia | UTF-8, UTF-16, and UTF-32 on StackOverflow | . ",
    "url": "/reference/encodings/#resources",
    "relUrl": "/reference/encodings/#resources"
  },"31": {
    "doc": "Fullscreen Decoder",
    "title": "Fullscreen Decoder",
    "content": "The fullscreen version: . | Does not resize content to fit your canvas | Allows you to pan and zoom | Has a larger canvas window | . ",
    "url": "/interactive/lz/fullscreen_decoder/",
    "relUrl": "/interactive/lz/fullscreen_decoder/"
  },"32": {
    "doc": "Fullscreen Encoder",
    "title": "Fullscreen Encoder",
    "content": "The fullscreen version: . | Does not resize content to fit your canvas | Allows you to pan and zoom | Has a larger canvas window | . ",
    "url": "/interactive/lz/fullscreen_encoder/",
    "relUrl": "/interactive/lz/fullscreen_encoder/"
  },"33": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "When jumping into this guide, it’s important to identify what you want to gain from it. For example, if you’d simply like to familiarize yourself with different compression algorithms, you could just skim over the different algorithms and play with the interactive algorithms. However, if you’re interested in understanding each algorithm at a deep level and creating your implementations, you’d likely want to take more time to read each page section-by-section. Though as a general guide, it’s a good idea to get started in the interactive algorithms section to get an idea of how each algorithm works visually by experimenting. After taking some time to see how each algorithm works, it’s a good idea to dive into the overview of algorithms and get started with an algorithm that interests you (Lempel-Ziv is a good starting choice). Lastly, you should familiarize yourself with the reference section and skim over any subjects you feel unfamiliar with. This section acts as a quick reference about different subjects that will inevitably come into play during the understanding and implementing of compression algorithms. Or, if you’d like to dive right in, you should start with Lempel-Ziv and Lempel-Ziv-Storer-Szymanski. ",
    "url": "/getting_started/",
    "relUrl": "/getting_started/"
  },"34": {
    "doc": "Tree Building Implementation",
    "title": "How To Build a Tree (Programmatically)",
    "content": " ",
    "url": "/algorithms/huffam-program/#how-to-build-a-tree-programmatically",
    "relUrl": "/algorithms/huffam-program/#how-to-build-a-tree-programmatically"
  },"35": {
    "doc": "Tree Building Implementation",
    "title": "General Implementation",
    "content": "Push an array of huffman leaf objects containing each character and its associated frequency into a priority queue. To start building the tree, pop two leafs from the queue and assign them as the left and right leafs for a node. Using this new node, push the node into the priority qeuue. Continue this process untile the size of the queue is 1. ",
    "url": "/algorithms/huffam-program/#general-implementation",
    "relUrl": "/algorithms/huffam-program/#general-implementation"
  },"36": {
    "doc": "Tree Building Implementation",
    "title": "Tree Building Implementation",
    "content": " ",
    "url": "/algorithms/huffam-program/",
    "relUrl": "/algorithms/huffam-program/"
  },"37": {
    "doc": "Huffman",
    "title": "Huffman",
    "content": "Since it’s creation by David A. Huffman in 1952, Huffman coding has been regarded as one of the most efficient and optimal methods of compression. Huffman’s optimal compression ratios are made possible through it’s character counting functionality. Unlike many algorithms in the Lempel-Ziv suite, Huffman encoders scan the file and generate a frequency table and tree before begining the true compression process. Before discussing different implementations, lets dive deeper into how the algorithm works. ",
    "url": "/algorithms/huffman/",
    "relUrl": "/algorithms/huffman/"
  },"38": {
    "doc": "Huffman",
    "title": "The Algorithm",
    "content": "Although huffman encoding may seem confusing from an outside view, we can break it into three simple steps: . | Frequency Counting . | Tree Building . | Character Encoding . | . ",
    "url": "/algorithms/huffman/#the-algorithm",
    "relUrl": "/algorithms/huffman/#the-algorithm"
  },"39": {
    "doc": "Huffman",
    "title": "Frequency Counting",
    "content": "Let’s start out by going over the frequency counting step. Throughout all of the examples, I will be using the following sample input string: . I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! . The huffman encoder starts out by going over the inputted text and outputing a table correlating each character to the number of time it appears in the text. For the sample input, the table would look this: . | Frequency | Character | . | 1 | N | . | 1 | \\n | . | 1 | K | . | 1 | D | . | 1 | L | . | 1 | E | . | 2 | O | . | 3 | H | . | 3 | ! | . | 3 | . | . | 6 | - | . | 6 | S | . | 7 | T | . | 8 | I | . | 12 | M | . | 15 | A | . | 17 |   | . As displayed above, the table is sorted to ensure consistency in each step of the compression process. ",
    "url": "/algorithms/huffman/#frequency-counting",
    "relUrl": "/algorithms/huffman/#frequency-counting"
  },"40": {
    "doc": "Huffman",
    "title": "Tree Building",
    "content": "Once the frequency table is created, the huffman encoder builds a huffman tree. A huffman tree follows the same structure as a normal binary tree, containing nodes and leafs. Each Huffman Leaf contains two values, the character and it’s corresponding frequency. To build the tree, we traverse our table of frequencies and characters, and push the characters with the highest frequencies to the top of tree. Continuing the traversal until each table value is represented on a Huffman Leaf. That might be confusing, so lets break it down step by step. Huffman compression works by taking existing 8 bit characters and assigning them to a smaller number of bits. To optimize the compression, the characters with the highest frequency are given smaller bit values. A Huffman Tree helps us assign and visualize the new bit value assigned to existing characters. Similar to a binary tree, if we start at the root node, we can traverse the tree by using 1 to move to the right and 0 to move to the left. The position of a leaf node relative the root node is used to determine it’s new bit value. A huffman tree for our example is depicted below: . As shown in the image, Huffman trees can get very large and complicated very easily. To see a sample tree for any text go to url. To understand more about the programatic implementation of tree building, click here. ",
    "url": "/algorithms/huffman/#tree-building",
    "relUrl": "/algorithms/huffman/#tree-building"
  },"41": {
    "doc": "Huffman",
    "title": "Character Encoding",
    "content": "Character encoding is the final step for most huffman encoders. Once a tree and frequency table has built, the final step is to encode the characters from the initial file and write the encoded bytes to a new file. This can be done in two ways. | Tree Traversal | Array Indexing | . Tree Traversal . Tree traversal is the first way of encoding the input of a huffman encoder. For each character, the tree is traversed recursively until a leaf with a matching character is found. This method can easily get complicated and very inefficient as the tree has to be traversed multiple times. For a simpler and quicker solution, we can use Array Indexing . Array Indexing . When compared to the previous tree traversal method, array indexing is much less complicated and significantly faster. Before encoding the characters, the tree is traversed once and the values for each leaf are outputted in two corresponding arrays. The first array contains the value of each character, while the second contains its updated bit value. Once created, the arrays are traversed and each character in the input is replaced with its updated bit value. Once a new output text is generated, it is encoded as a byte array and written to the output file. ",
    "url": "/algorithms/huffman/#character-encoding",
    "relUrl": "/algorithms/huffman/#character-encoding"
  },"42": {
    "doc": "Overview",
    "title": "The Hitchhiker’s Guide to Compression",
    "content": "Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea. This planet has a problem, which was this: files are too big. Many solutions were suggested for solving this problem via lossless compression, such as Lempel-Ziv and Huffman coding, but most of these were implemented into common compression utilities and promptly forgotten. Today, much of the relevant work to compression is in an obscure corner of the internet between lengthy PhD thesis papers and hard-to-find gems. ",
    "url": "/#the-hitchhikers-guide-to-compression",
    "relUrl": "/#the-hitchhikers-guide-to-compression"
  },"43": {
    "doc": "Overview",
    "title": "Why compression",
    "content": "Lossless file compression, and file compression in general has become a lost art. The modern developer community has moved on from working on compression algorithms to bigger and better problems, such as creating the next major NodeJS framework. However, compression as it stands in the computer science aspect is still as interesting as it was in 1980s, possibly even more so today with an estimated 463 Exabytes of data to be created everyday in 2025. It’s no secret that the internet is growing rapidly, and with it more people are becoming connected. From urban dwellers to rural farmers, fast internet speeds are not a given. To counter this, there are numerous projects focused on improving internet speeds for rural users, but there are far fewer projects focused on the other half of improving internet access: compressing data. These claims about the “lost of art of compression” may seem a bit unsubstantiated, as there are new and actively developed compression projects out there today, such as, but not limited to: . | Facebook’s ZSTD | Google’s Brotli | LZ4 | Shrynk | . There are also some other notable projects which I’ve included at the end, but either they aren’t active or universal enough to be included in this short list. However this argument still holds true, compression isn’t really mainstream, and I don’t know why it isn’t. Internet speeds are a real problem for much of the world and better compression stands as a promising solution. The possibilities of better compression are truly endless: . | Faster 4k video streaming | Faster download speeds | Faster upload speeds | Faster website and content loading speeds | Better accessibility to services in regions with poor internet connections | Higher quality video and voice calls | Less expensive deep data storage costs | and more | . ",
    "url": "/#why-compression",
    "relUrl": "/#why-compression"
  },"44": {
    "doc": "Overview",
    "title": "The Goal",
    "content": "The goal of this project, and by extension, the goal of all resources here is to help people learn about compression algorithms and encourage people to tinker, build, and experiment with their own algorithms and implementations. Afterall, the best way to innovate in tech is to get a bunch of developers interested in something and let them lead the way. Additionally, this project itself is intended to be a community-sourced resource for people interested in compression algorithms. The idea is that anyone can contribute to this website through GitHub so that this can be a constantly improving and expanding resource for others. With all of that said, if you’re interested in learning more about the world of compression, you should get started. ",
    "url": "/#the-goal",
    "relUrl": "/#the-goal"
  },"45": {
    "doc": "Overview",
    "title": "Forewarning",
    "content": "As a forewarning it should be noted that although this project is intended to be comprehensive, it is not. This is because the two primary authors, myself, Phillip Cutter, and Arnav Chawla, are currently high school seniors who are working on finishing college applications, schoolwork, internships, and other commitments and do not have the time to continue this project, and Raisin, at the same rate that they did over the first three months. However, both of us are committed to maintaining this project and improving it for the forseeable future. ",
    "url": "/#forewarning",
    "relUrl": "/#forewarning"
  },"46": {
    "doc": "Overview",
    "title": "Notable Compression Project Mentions",
    "content": "Notable mentions: . | LZHAM (not super active) | Google’s WebP (only for images) | Dropbox’s DivANS | Dropbox’s avrecode | Dropbox’s Lepton | H.266/VCC Codec (domain-specific for video) | Pied Piper Middle-Out (abandoned and closed-source unfortunately) | . ",
    "url": "/#notable-compression-project-mentions",
    "relUrl": "/#notable-compression-project-mentions"
  },"47": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"48": {
    "doc": "Interactive Algorithms",
    "title": "Interactive Algorithms",
    "content": "This section serves as a collection of interactive algorithms that make it easier to experiment and visually see how an data compression algorithm works. ",
    "url": "/interactive/interactive/",
    "relUrl": "/interactive/interactive/"
  },"49": {
    "doc": "LZ77/LZSS",
    "title": "LZ77/LZSS",
    "content": " ",
    "url": "/interactive/lz/lz/",
    "relUrl": "/interactive/lz/lz/"
  },"50": {
    "doc": "Lempel-Ziv",
    "title": "Lempel-Ziv",
    "content": "Lempel-Ziv, commonly referred to as LZ77/LZ78 depending on the variant, is one of the oldest, most simplistic, and widespread compression algorithms out there. Its power comes from its simplicity, speed, and decent compression rates. Now before we dive into an implementation, let’s understand the concept behind Lempel-Ziv and the various algorithms it has spawned. ",
    "url": "/algorithms/lz/",
    "relUrl": "/algorithms/lz/"
  },"51": {
    "doc": "Lempel-Ziv",
    "title": "The Algorithm(s)",
    "content": "Lempel-Ziv at its core is very simple. It works by taking an input string of characters, finding repetitive characters, and outputting an “encoded” version. To get an idea of it, here’s an example. Original: Hello everyone! Hello world! Encoded: Hello everyone! &lt;16,6&gt;world! . As you can see, the algorithm simply takes an input string, in this case, “Hello everyone! Hello world!”, and encodes it character by character. If it tries to encode a character it has already seen it will check to see if it has seen the next character. This repeats until it the character it’s checking hasn’t been seen before, following the characters it’s currently encoding, at this point it outputs a “token”, which is &lt;16,6&gt; in this example, and continues. The &lt;16,6&gt; token is quite simple to understand too, it consists of two numbers and some syntactic sugar to make it easy to understand. The first number corresponds to how many characters it should look backwards, and the next number tells it how many characters to go forwards and copy. This means that in our example, &lt;16,6&gt; expands into “Hello “ as it goes 16 characters backwards, and copies the next 6 characters. This is the essential idea behind the algorithm, however it should be noted that there are many variations of this algorithm with different names. For example, in some implementations, the first number means go forwards from the beginning instead of backwards from the current position. Small (and big) differences like these are the reason for so many variations: . | LZSS - Lempel-Ziv-Storer-Szymanski | LZW - Lempel-Ziv-Welch | LZMA - Lempel–Ziv–Markov chain algorithm | LZ77 - Lempel-Ziv 77 | LZ78 - Lempel-Ziv 78 | . It’s also important to understand the difference between LZ77 and LZ78, the first two Lempel-Ziv algorithms. LZ77 works very similarly to the example above, using a token to represent an offset and length, while LZ78 uses a more complicated dictionary approach. For a more in-depth explanation, make sure to check out this wonderful article explaining LZ78. ",
    "url": "/algorithms/lz/#the-algorithms",
    "relUrl": "/algorithms/lz/#the-algorithms"
  },"52": {
    "doc": "Lempel-Ziv",
    "title": "Implementations",
    "content": "Now because there are so many different variations of Lempel-Ziv algorithms, there isn’t a single “LZ” implementation. WIth that being said, if you are interested in implementing a Lempel-Ziv algorithm yourself, you’ll have to choose an algorithm to start with. LZSS is a great jumping-off point as it’s a basic evolution of LZ77 and can be implemented very easily while achieving a respectable compression ratio. If you’re interested in another algorithm, head back to the algorithms overview. ",
    "url": "/algorithms/lz/#implementations",
    "relUrl": "/algorithms/lz/#implementations"
  },"53": {
    "doc": "LZSS",
    "title": "Lempel-Ziv-Storer-Szymanski",
    "content": "Lempel-Ziv-Storer-Szymanski, which we’ll refer to as LZSS, is a simple variation of the common LZ77 algorithm. It uses the same token concept with an offset and length to tell the decoder where to copy the text, except it only places the token when the token is shorter than the text it is replacing. The idea behind this is that it will never increase the size of a file by adding tokens everywhere for repeated letters. You can imagine that LZ77 would easily increase the file size if it simply encoded every repeated letter “e” or “i” as a token, which may take at least 5 bytes depending on the file and implementation instead of just 1 for LZSS. ",
    "url": "/algorithms/lzss/#lempel-ziv-storer-szymanski",
    "relUrl": "/algorithms/lzss/#lempel-ziv-storer-szymanski"
  },"54": {
    "doc": "LZSS",
    "title": "Implementing an Encoder",
    "content": "Let’s take a look at some examples, so we can see exactly how it works. The wikipedia article for LZSS has a great example for this, which I’ll use here, and it’s worth a read as an introduction to LZSS. So let’s encode an excerpt of Dr. Seuss’s Green Eggs and Ham with LZSS (credit to Wikipedia for this example). I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! DO WOULD YOU LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM,SAM-I-AM. I DO NOT LIKE GREEN EGGS AND HAM. This text takes up 192 bytes in a typical UTF-8 encoding. Let’s take a look at the LZSS encoded version. I AM SAM. &lt;10,10&gt;SAM I AM. THAT SAM-I-AM! T&lt;15,14&gt;I DO NOT LIKE&lt;29,15&gt; DO WOULD YOU LIKE GREEN EGGS AND HAM? I&lt;69,15&gt;EM,&lt;113,8&gt;.&lt;29,15&gt;GR&lt;64,16&gt;. This encoded, or compressed, version only takes 148 bytes to store (without a magic type to describe the file type), which is 77% of the original file size, or a compression ratio of 1.3. Not bad! . Analysis . Now let’s take a second to understand what’s happening before you start trying to conquer the world with LZSS. As we can see, the “tokens” are reducing the size of the file by referencing pieces of text that are longer than the actual token. Let’s look at the first line: . I AM SAM. &lt;10,10&gt;SAM I AM. The encoder works character by character. On the first character, ‘I’, it checks it’s search buffer to see if it’s already seen an ‘I’. The search buffer is essentially the encoder’s memory, for every character it encodes, it adds it into the search buffer so it can “remember” it. Because it hasn’t seen an ‘I’ already (the search buffer is empty), it just outputs an ‘I’, adds it to the search buffer, and moves to the next character. The next character is ‘ ‘ (a space). The encoder checks the search buffer to see if it’s seen a space before, and it hasn’t so it outputs the space and moves forward. Once it gets to the second space (after “I AM”), LZ77 comes into play. It’s already seen a space before because it’s in the search buffer so it’s ready to output a token, but first it tries to maximize how much text the token is referencing. If it didn’t do this you could imagine that for every character it’s already seen it would output something similar to &lt;5,1&gt;, which is 5 times larger than any character. So once it finds a character that it’s already seen, it moves on to the next character and checks if it’s already seen the next character directly after the previous character. Once it finds a sequence of characters that it hasn’t already seen, then it goes back one character to the sequence of characters it’s already seen and prepares the token. Once the token is ready, the difference between LZ77 and LZSS starts to shine. At this point LZ77 simply outputs the token, adds the characters to the search buffer and continues. LZSS does something a little smarter, it will check to see if the size of the outputted token is larger than the text it’s representing. If so, it will output the text it represents, not the token, add the text to the search buffer, and continue. If not, it will output the token, add the text it represents to the search buffer and continue. Coming back to our example, the space character has already been seen, but a space followed by an “S” hasn’t been seen yet (“ S”), so we prepare the token representing the space. The token in our case would be “&lt;3,1&gt;”, which means go back three characters and copy 1 character(s). Next we check to see if our token is longer than our text, and “&lt;3,1&gt;” is indeed longer than “ “, so it wouldn’t make sense to output the token, so we output the space, add it to our search buffer, and continue. This entire process continues until we get to the “I AM SAM. “. At this point we’ve already seen an “I AM SAM. “ but haven’t seen an “I AM SAM. S” so we know our token will represent “I AM SAM. “. Then we check to see if “I AM SAM. “ is longer than “&lt;10,10&gt;”, which it is, so we output the token, add the text to our search buffer and continue along. This process continues, encoding tokens and adding text to the search buffer character by character until it’s finished encoding everything. Takeaways . There’s a lot of information to unpack here, but the algorithm at a high level is quite simple: . | Loop character by character | Check if it’s seen the character before . | If so, check the next character and prepare a token to be outputted . | If the token is longer than the text it’s representing, don’t output a token | Add the text to the search buffer and continue | . | If not, add the character to the search buffer and continue | . | . It’s important to remember that no matter the outcome, token or no token, the text is always appended to the search buffer so it can always “remember” the text it’s already seen. ",
    "url": "/algorithms/lzss/#implementing-an-encoder",
    "relUrl": "/algorithms/lzss/#implementing-an-encoder"
  },"55": {
    "doc": "LZSS",
    "title": "Implementation",
    "content": "Now let’s take a stab at building our very own version so we can understand it more deeply. As with most of these algorithms, we have an implementation written in Go in our raisin project. If you’re interested in what a more performant or real-world example of these algorithms looks like, be sure to check it out. However for this guide we’ll use Python to make it more approachable so we can focus on understanding the algorithm and not the nuances of the language. Character Loop . Let’s get started with a simple loop that goes over each character for encoding. As we can see from our takeaways, the character-by-character loop is what powers LZSS. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character . Output: . H E L L O . Although the code is functionally pretty simple, there’s a few important things going on here. You can see that looping character-by-character isn’t as simple as for char in text, first we have to encode it and then loop over the encoding. This is because it converts our string into an array of bytes, represented as a Python object called bytes. When we print the character out, we have to convert it from a byte (represented as a Python int) back to a string so we can see it. The reason we do this is because a byte is really just a number from 0-255 as it is represented in your computer as 8 1’s and 0’s, called binary. If you don’t already have a basic understanding of how computers store our language, you should get acquainted with it on our encodings page. Search Buffers . Great, we have a basic program working which can loop over our text and print it out, but that’s pretty far off from compression so let’s keep going. The next step to our program is to implement our “memory” so the program can check to see if its already seen a character. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . We no longer need to output anything as we’re just adding each character to the search buffer with the append method. Checking Our Search buffer . Now let’s try to implement the LZ part of LZSS, we need to start looking backwards for characters we’ve already seen. This can accomplished quite easily using the list.index method. for char in text_bytes: if char in search_buffer: print(f\"Found at {search_buffer.index(char)}\") print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . Output: . H E L Found at 2 L O . Notice the if char in search_buffer, without this Python will throw an IndexError if the value is not in the list. Building Tokens . Now let’s build a token and output it when we find the character. i = 0 for char in text_bytes: if char in search_buffer: index = search_buffer.index(char) # The index where the character appears in our search buffer offset = i - index # Calculate the relative offset length = 1 # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . H E L &lt;1,1&gt; O . We’re nearly there! This is actually a rough implementation of LZ77, however there’s one issue. If we have a word that repeats twice, it will copy each character instead of the entire word. text = \"SAM SAM\" . Output . S A M &lt;4,1&gt; &lt;4,1&gt; &lt;4,1&gt; . Note: &lt;4,1&gt; is technically correct as each character is represented 4 characters behind the beginning of the token. That’s not exactly right, we should see &lt;4,3&gt; instead of three &lt;4,1&gt; tokens. So let’s write some code that can check our search buffer for more than one character. Checking the Search Buffer for More Characters . Let’s modify our code to check the search buffer for more than one character. def elements_in_array(check_elements, elements): i = 0 offset = 0 for element in elements: if len(check_elements) &lt;= offset: # All of the elements in check_elements are in elements return i - len(check_elements) if check_elements[offset] == element: offset += 1 else: offset = 0 i += 1 return -1 text = \"SAM SAM\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M &lt;4,3&gt; . It works! But there’s quite a lot to unpack here so let’s go through it line by line. The first and largest addition is the elements_in_array function. The code here essentially checks to see if specific elements are within an array in an exact order. If so, it will return the index in the array where the elements start, and if not it will return -1. Moving on to our main function loop we can see now have check_characters defined at the top. This variable tracks what characters we’re looking for in our search_buffer. As we loop through, we use check_characters.append(char) to add the current character to the characters we’re searching. Then we check to see if check_characters can be found within search_buffer with elements_in_array. Now we have the best part: the logic. If we couldn’t find a match or it’s the last character we want to output something. If we couldn’t find more than one character in the search_buffer then that means check_characters minus the last character was found, so we’ll output a token representing check_characters minus the last character. Otherwise, we couldn’t find a match for a single character so let’s just output that character. And that’s essentially LZ77! Try it out for yourself with some different strings to see for yourself. However, you might notice that we’re trying to implement LZSS, not LZ77, so we have one more piece to implement. Comparing Token Sizes . This crucial piece is the process described earlier of comparing the size of tokens versus the text it represents. Essentially we’re saying, if the token takes up more space than the text it’s representing then don’t output a token, just output the text. Lucky for us this is a pretty simple change. Our main loop now looks like so: . for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character print(bytes(check_characters).decode(encoding)) # Print the characters else: print(token) # Print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M SAM . The key is the len(token) &gt; length which checks if the length of the token is longer than the length of the text it’s representing. If it is, it simply outputs the characters, otherwise it outputs the token. Sliding Windows . The last piece to the puzzle is something you might have noticed if you’re already trying to compress large file: the search buffer gets big. Let’s say we’re compressing a 1 Gb file. After we go over each character, we add it to the search buffer and continue, though each iteration we also search the entire search buffer for certain characters. This quickly adds up for larger files. In our 1 Gb file scenario, near the end we’ll have to search almost 1 billion bytes to encode a single character. It should be pretty obvious that this very inefficient. And unfortunately, there is no magic solution. You have to make a tradeoff. With every compression algorithm you have to decide between speed and compression ratio. Do you want a fast algorithm that can’t reduce the file size very much, or a slow algorithm that reduces the file size more? The answer is: it depends. And so, the tradeoff in LZ77’s case is to create a “sliding window”. The “sliding window” is actually quite simple, all you do is cap off the maximum size of the search buffer. When you add a character to the search buffer that makes it larger than the maximum size of the sliding window then you remove the first character. That way the window is “sliding” as you move through the file, and the algorithm doesn’t slow down! . max_sliding_window_size = 4096 ... for char in text_bytes: ... search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer ... These changes should be pretty self-explanatory. We’re just checking to see if the length of the search_buffer is greater than the max_sliding_window_size, and if so we pop the first element off of the search_buffer. Keep in mind that while a maximum sliding window size of 4096 character is typical, it may be hard to use during testing, try setting it much lower (like 3-4) and test it with some different strings to see how it works. Putting it all together . That’s everything that makes up LZSS, but for the sake of completing our example, let’s clean it up so we can call a function with some text, an optional max_sliding_window_size, and have it return the encoded text, rather than just printing it out. encoding = \"utf-8\" def encode(text, max_sliding_window_size=4096): text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes output = [] # Output array i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token else: output.extend(check_characters) # Output the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 return bytes(output) print(encode(\"SAM SAM\", 1).decode(encoding)) print(encode(\"supercalifragilisticexpialidocious supercalifragilisticexpialidocious\", 1024).decode(encoding)) print(encode(\"LZSS will take over the world!\", 256).decode(encoding)) print(encode(\"It even works with 😀s thanks to UTF-8\", 16).decode(encoding)) . The function definition is pretty simple, we can just move our text and max_sliding_window_size outside of the function and wrap our code in a function definition. Then we simply call it with some different values to test it, and that’s it! . The finished code can be found in lzss.py in the examples GitHub repo. Lastly, there’s a few bugs in our program that we encounter with larger files. If we have some text, for example: . ISAM YAM SAM . When the encoder gets to the space right before the “SAM”, it will look for a space in the search buffer which it finds. Then it will search for a space and an “S” (“ S”) which it doesn’t find, so it continues and starts looking for an “A”. The issue here is that it skips looking for an “S” and continues to encode the “AM” not the “SAM”. In some rare circumstances the code may generate a reference with a length that is larger than its offset which will result in an error. To fix this, we’ll need to rewrite the logic in our encoder a little bit. for char in text_bytes: index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if elements_in_array(check_characters + [char], search_buffer) == -1 or i == len(text_bytes) - 1: if i == len(text_bytes) - 1 and elements_in_array(check_characters + [char], search_buffer) != -1: # Only if it's the last character then add the next character to the text the token is representing check_characters.append(char) if len(check_characters) &gt; 1: index = elements_in_array(check_characters, search_buffer) offset = i - index - len(check_characters) # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the characters output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token search_buffer.extend(check_characters) # Add the characters to our search buffer else: output.extend(check_characters) # Output the character search_buffer.extend(check_characters) # Add the characters to our search buffer check_characters = [] check_characters.append(char) if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 . To fix the first issue we add the current char to check_characters only at the end and check to see if check_characters + [char] is found. If not we know that check_characters is found so we can continue as normal, and check_characters gets cleared before char is added onto check_characters for the next iteration. We also implement a check on the last iteration to add the current char to check_characters as otherwise our logic wouldn’t be run on the last character and a token wouldn’t be created. To resolve the other problem we simply have to move the search_buffer.append(char) calls up into our logic and change them to search_buffer.extend(check_characters). This way we only update our search buffer when we’ve already tried to find a token. ",
    "url": "/algorithms/lzss/#implementation",
    "relUrl": "/algorithms/lzss/#implementation"
  },"56": {
    "doc": "LZSS",
    "title": "Implementing a Decoder",
    "content": "What’s the use of encoding something some text if we can’t decode it? For that we’ll need to build ourselves a decoder. Luckily for us, building a decoder is actually much simpler than an encoder because all it needs to know how to do is convert a token (“&lt;5,2&gt;”) into the literal text it represents. The decoder doesn’t care about search buffers, sliding windows, or token lengths, it has only one job. So, let’s get started. We’re going to decode character-by-character just like our encoder so we’ll start with our main loop inside of a function. We’ll also need to encode and decode the strings so we’ll keep the encoding = \"utf-8\". encoding = \"utf-8\" def decode(text): text_bytes = text.encode(encoding) # The text encoded as bytes output = [] # The output characters for char in text_bytes: output.append(char) # Add the character to our output return bytes(output) print(decode(\"supercalifragilisticexpialidocious &lt;35,34&gt;\").decode(encoding)) . Here we’re setting up the structure for the rest of our decoder by setting up our main loop and declaring everything within a neat self-contained function. Identifying Tokens . The next step is to start doing some actual decoding. The goal of our decoder is to convert a token into text, so we need to first identify a token and extract our offset and length before we can convert it into text. Notice the various components of a token that need to be identified and extracted so we can find the text they represent . Let’s make a small change so we can identify the start and end of a token. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: print(\"Found opening of a token\") elif char == \"&gt;\".encode(encoding)[0]: print(\"Found closing of a token\") output.append(char) # Add the character to our output return bytes(output) . Because we’re going character-by-character we can simply check to see if the character is a token opening character or closing character to tell if we’re inside a token. Let’s add some more code to track the numbers between the comma, our seperator. inside_token = False scanning_offset = True length = [] # Length number encoded as bytes offset = [] # Offset number encoded as bytes for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) print(f\"Found token with length: {length_num}, offset: {offset_num}\") # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) output.append(char) # Add the character to our output . Output: . Found token with length: 34, offset: 35 supercalifragilisticexpialidocious &lt;35,34&gt; . We now have a bunch of if statements that give our loop some more control flow. Let’s go over the changes. First off we have four new variables outside of the loop: . | inside_token - Tracks whether or not we’re inside a token | scanning_offset - Tracks whether we’re currently scanning for the offset number or the length number (1st or 2nd number in the token) | length - Used to store the bytes (or characters) that represent the token’s length | offset- Used to store the bytes (or characters) that represent the token’s offset | . Inside of the loop, we check if the character is a &lt;, ,, or a &gt; and modify the variables accordingly to track where we are. If the character isn’t any of those and we’re inside a token then we want to add the character to either the offset or length because that means the character is an offset or length. Lastly, if the character is a &gt;, that means we’re exiting the token, so let’s convert our length and offset into a Python int. We have to do this because they’re currently represented as a list of bytes, so we need to convert those bytes into a Python string and convert that string into an int. Then we finally print that we’ve found a token. Translating Tokens . Now we have one last step left: translating tokens into the text they represent. Thanks to Python list slicing this is quite simple. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number token_start = i elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) # Get text that the token represents referenced_text = output[-offset_num:][:length_num] output.extend(referenced_text) # referenced_text is a list of bytes so we use extend to add each one to output # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) else: output.append(char) # Add the character to our output return bytes(output) . Output: . supercalifragilisticexpialidocious supercalifragilisticexpialidocious . In order to calculate the piece of text that a token is referencing we can simply use our offset and length to find the text from the current output. We use a negative slice to grab all the characters backwards from offset_num and grab up to length_num elements. This results in a referenced_text that represents the token references. Finally we add the referenced_text to our output and we’re finished. Lastly, we’ll only want to add a character to the output if we’re not in a token so we add an else to the end of our logic which only runs if we’re not in a token. And that’s it! We now have a LZSS decoder, and by extension, an LZ77 decoder as decoders don’t need to worry about outputting a token only if it’s greater than the referenced text. ",
    "url": "/algorithms/lzss/#implementing-a-decoder",
    "relUrl": "/algorithms/lzss/#implementing-a-decoder"
  },"57": {
    "doc": "LZSS",
    "title": "Implementation Conclusion",
    "content": "We’ve gone through step-by-step building an encoder and decoder and learned the purpose of each component. Let’s do some basic benchmarks to see how well it works. if __name__ == \"__main__\": encoded = encode(text).decode(encoding) decoded = decode(encoded).decode(encoding) print(f\"Original: {len(text)}, Encoded: {len(encoded)}, Decoded: {len(decoded)}\") print(f\"Lossless: {text == decoded}\") print(f\"Compressed size: {(len(encoded)/len(text)) * 100}%\") . Using the text as Green Eggs and Ham by Doctor Seuss, we see the output: . Original: 3463 bytes, Encoded: 1912 bytes, Decoded: 3463 bytes Lossless: True Compressed size: 55.21224371931851% . LZSS just reduced the file size by 45%, not bad! . One thing to keep in mind is that when we refer to a “character”, we really mean a “byte”. Our loop runs byte-by-byte, not character-by-character. This distinction is minor but significant. In the world of encodings, not every character is a single byte. For example in utf-8, any english letter or symbol is a single byte, but more complicated characters like arabic, mandarin, or emoji characters require multiple bytes despite being a single “character”. | 4 bytes - 😀 | 1 byte - H | 3 bytes - 话 | 6 bytes - يَّ | . If you’re interested in learning more about how bytes work, check out the Wikipedia articles on Bytes and Unicode or our reference page on bytes. ",
    "url": "/algorithms/lzss/#implementation-conclusion",
    "relUrl": "/algorithms/lzss/#implementation-conclusion"
  },"58": {
    "doc": "LZSS",
    "title": "LZSS",
    "content": " ",
    "url": "/algorithms/lzss/",
    "relUrl": "/algorithms/lzss/"
  },"59": {
    "doc": "Unix Magic Numbers",
    "title": "Unix Magic Numbers",
    "content": "Unix magic types, also known as magic numbers, or format indicators are a small sequence of bytes at the beginning of a file used to identify the file type. Here are just a few examples of file magic numbers from Wikipedia: . | GIFs: GIF89a | Unix scripts: #! | PDFs: 25 50 44 46 | JPEGs: FF D8 | . Magic numbers are important to understand if you start writing and reading compressed files. Written compressed files should include your own custom magic number that represents your compression format and doesn’t conflict with any other magic number. You can also read these magic types to determine the format of a compressed file. These magic numbers and the corresponding file type are generally stored in a file called the magic patterns, also known as the magic database or magic data. This file is used by Unix systems to lookup the file type based off of a file’s magic numbers. For a more complete list of different magic numbers, check out the Wikipedia entry for a list of file extensions. ",
    "url": "/reference/magic_numbers/",
    "relUrl": "/reference/magic_numbers/"
  },"60": {
    "doc": "Unix Magic Numbers",
    "title": "Resources",
    "content": "To learn more about magic numbers, check out these resources: . | Linux man page for file | List of file signatures on Wikipedia | Format indicators on Wikipedia | Working with Magic numbers in Linux | . ",
    "url": "/reference/magic_numbers/#resources",
    "relUrl": "/reference/magic_numbers/#resources"
  },"61": {
    "doc": "Overview of Algorithms",
    "title": "Overview of Algorithms",
    "content": "The following is intended to be a comprehensive list of lossless compression algorithms (in no particular order), however if you feel like an algorithm is missing, please let us know. | Run-length Coding | Range Coding | Lempel-Ziv . | LZ77 | LZ78 | LZSS | LZW | . | Dictionary Coding | Variable-length Coding | Huffman Coding | Arithmetic Coding | Dynamic Markov Compression | FLATE | . For a more complete list, check out these Wikipedia pages on lossless algorithms and lossy algorithms. ",
    "url": "/algorithms/overview/",
    "relUrl": "/algorithms/overview/"
  },"62": {
    "doc": "Raisin",
    "title": "Raisin",
    "content": "Raisin is a compression project that myself, Phillip Cutter, and Arnav Chawla built under the guidance of our mentor, Skip Tavakkolian. This project spanned 6 weeks and encompasses our work in implementing various compression algorithms in Go along with an easy-to-use CLI interface. This website is the project encompassing our learning along our journey of researching and implementing lossless compression algorithms. Most compression algorithm documents on this site should be implemented within this project in a performant manner, though if not, you should check the examples repo. ",
    "url": "/reference/raisin/",
    "relUrl": "/reference/raisin/"
  },"63": {
    "doc": "Reference",
    "title": "Reference",
    "content": "This section serves to be a reference to topics that aren’t directly related to compression, but inveitably come into play. A basic understanding of these concepts are invaluable when building your own implementation or algorithm or simply understanding how these algorithms work. ",
    "url": "/reference/reference/",
    "relUrl": "/reference/reference/"
  },"64": {
    "doc": "Resources",
    "title": "Resources",
    "content": "This is a list of great resources for learning about compression in a broad sense. If you’re looking for other places to learn more about data compression then any of these entries are a great start. | Compressor Head - Google Compression Show | mathematicalmonk on Information Theory - Very in-depth and comprehensive YouTube lectures | Data compression on Wikipedia - A good rabbit hole on Wikipedia | GitHub compression-algorithm Topic - The latest GitHub compression projects | . Know another great resource not included on this list? Consider contributing. ",
    "url": "/resources/",
    "relUrl": "/resources/"
  }
}
