{"0": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": "This site is intended to be used as a living documentation that is constantly updated by the community, if you feel like something’s missing or would like to contribute, please do one of the following create an issue or create a pull request. The best way to ask a question is to file an issue and we’ll try to get back to you. ",
    "url": "https://go-compression.github.io//contributing.html",
    "relUrl": "/contributing.html"
  },"1": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "This article is currently a stub. ",
    "url": "https://go-compression.github.io//getting_started.html",
    "relUrl": "/getting_started.html"
  },"2": {
    "doc": "Overview",
    "title": "The Hitchhiker’s Guide to Compression",
    "content": "Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape- descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea. This planet has - or rather had - a problem, which was this: files were too big. Many solutions were suggested for solving this problem via lossless compression, such as Lempel-Ziv and Huffman coding, but most of these were implemented into common compression utilities and promptly forgotten. Today, much of the relevant work to compression is in an obscure corner of the internet between lengthy PhD thesis papers and hard-to-find gems. ",
    "url": "https://go-compression.github.io//#the-hitchhikers-guide-to-compression",
    "relUrl": "/#the-hitchhikers-guide-to-compression"
  },"3": {
    "doc": "Overview",
    "title": "Why compression",
    "content": "Lossless file compression, and file compression in general has become a lost art. The modern developer community has moved on from working on compression algorithms to bigger and better problems, such as creating the next major NodeJS framework. However, compression as it stands in the computer science aspect is still as interesting as it was in 1980s, possibly even more so today with an estimated 463 Exabytes of data to be created everyday in 2025. It’s no secret that the internet is growing rapidly, and with it more people are becoming connected. From urban dwellers to rural farmers, fast internet speed are not a given. To counter this, there are numerous projects focused on improving internet speeds for rural users, but there are almost no projects focused on the other half of improving internet access: compressing data. These claims about the “lost of art of compression” may seem a bit unsubstantiated, as there are new and actively developed compression projects out there today, such as, but not limited to: . | Facebook’s ZSTD | Google’s Brotli | LZ4 | Shrynk | . However this argument still holds true, compression isn’t really mainstream, and I don’t know why it isn’t. Internet speeds is a real problem and better compression stands as a promising solution. The possibilities of better compression are truly endless: . | Faster 4k video streaming | Faster app downloads | Less delay loading websites and content | and more | . ",
    "url": "https://go-compression.github.io//#why-compression",
    "relUrl": "/#why-compression"
  },"4": {
    "doc": "Overview",
    "title": "The Goal",
    "content": "The goal of this project, and by extension, the goal of all resources here is to help people learn about compression algorithms and encourage people to tinker, build, and experiment with their own algorithms and implementations. Afterall, the best way to innovate in tech is to get a bunch of developers interested in something and let them lead the way. Additionally, this project itself is intended to be a community-sourced resource for people interested in compression algorithms. The idea is that anyone can contribute to this website through GitHub so that this can be a constantly improving and expanding resource for others. With all of that said, if you’re interested in learning more about the world of compression, you should get started. ",
    "url": "https://go-compression.github.io//#the-goal",
    "relUrl": "/#the-goal"
  },"5": {
    "doc": "Overview",
    "title": "Notable Compression Project Mentions",
    "content": "There are also some other notable projects which I’ve included at the end, but either they aren’t active enough or univeral to be included here. Notable mentions: . | LZHAM (not super active) | Google’s WebP (only for images) | Dropbox’s DivANS | Dropbox’s avrecode | H.266/VCC Codec (domain-specific for video) | Pied Piper Middle-Out (abandoned and closed-source unfortunately) | . ",
    "url": "https://go-compression.github.io//#notable-compression-project-mentions",
    "relUrl": "/#notable-compression-project-mentions"
  },"6": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "https://go-compression.github.io//",
    "relUrl": "/"
  },"7": {
    "doc": "Lempel-Ziv",
    "title": "Lempel-Ziv",
    "content": "Lempel-Ziv, commonly referred to as LZ77/LZ78 depending on the variant, is one of the oldest, most simplistic, and widespread compression algorithms out there. It’s power comes from its simplicity, speed, and decent compression rates. Now before we dive into an implementation, let’s understand the concept behind Lempel-Ziv and the various algorithms it has spawned. ",
    "url": "https://go-compression.github.io//algorithms/lz.html",
    "relUrl": "/algorithms/lz.html"
  },"8": {
    "doc": "Lempel-Ziv",
    "title": "The Algorithm(s)",
    "content": "Lempel-Ziv at its core is very simple. It works by taking an input string of characters, finding repetitive characters, and outputting an “encoded” version. To get an idea of it, here’s an example. Original: Hello everyone! Hello world! Encoded: Hello everyone! &lt;16,6&gt;world! . As you can see, the algorithm simply takes an input string, in this case, “Hello everyone! Hello world!”, and encodes it character by character. If it tries to encode a character it has already seen it will check to see if it has seen the next character. This repeats until it the character it’s checking hasn’t been seen before, following the characters it’s currently encoding, at this point it outputs a “token”, which is &lt;16,6&gt; in this example, and continues. The &lt;16,6&gt; token is quite simple to understand too, it consists of two numbers and some syntactic sugar to make it easy to understand. The first number corresponds to how many characters it shoud look backwards, and the next number tells it how many characters to go forwards and copy. This means that in our example, &lt;16,6&gt; expands into “Hello “ as it goes 16 characters backwards, and copies the next 6 characters. This is the essential idea behind the algorithm, however it should be noted that there are many variations of this algorithm with different names. For example, in some implementations, the first number means go forwards from the beginning instead of backwards from the current position. Small (and big) differences like these are the reason for so many variations: . | LZSS - Lempel-Ziv-Storer-Szymanski | LZW - Lempel-Ziv-Welch | LZMA - Lempel–Ziv–Markov chain algorithm | LZ77 - Lempel-Ziv 77 | LZ78 - Lempel-Ziv 78 | . It’s also important to understand the difference between LZ77 and LZ78, the first, and most common, Lempel-Ziv algorithms. LZ77 works very similarly to the example above, using a token to represent an offset and length, while LZ78 uses a more complicated dictionary approach. For a more in-depth explanation, make sure to check out this wonderful article explaining LZ78. UNFINISHED . ",
    "url": "https://go-compression.github.io//algorithms/lz.html#the-algorithms",
    "relUrl": "/algorithms/lz.html#the-algorithms"
  },"9": {
    "doc": "Lempel-Ziv",
    "title": "Implementations",
    "content": "Now because there are so many different variations of Lempel-Ziv algorithms, there isn’t a single LZ implementation. WIth that being said, if you are interested in implementing a Lempel-Ziv algorithm yourself, you’ll have to choose an algorithm to start with. LZSS is a great starting point as it’s a basic evolution of LZ77 and can be implemented very easily while achieving a respectable compression ratio. If you’re interested in another algorithm, head back to the algorithms . ",
    "url": "https://go-compression.github.io//algorithms/lz.html#implementations",
    "relUrl": "/algorithms/lz.html#implementations"
  },"10": {
    "doc": "LZSS",
    "title": "Lempel-Ziv-Storer-Szymanski",
    "content": "Lempel-Ziv-Storer-Szymanski, which we’ll refer to as LZSS, is a simple variation of the common LZ77 algorithm. It uses the same token concept with an offset and length to tell the decoder where to copy the text, except it only places the token when the token is shorter than the text it is replacing. The idea behind this is that it will never increase the size of a file by adding tokens everywhere for repeated letters. You can imagine that LZ77 would easily increase the file size if it simply encoded every repeated letter “e” or “i” as a token, which may take at least 5 bytes depending on the file and implementation. ",
    "url": "https://go-compression.github.io//algorithms/lzss.html#lempel-ziv-storer-szymanski",
    "relUrl": "/algorithms/lzss.html#lempel-ziv-storer-szymanski"
  },"11": {
    "doc": "LZSS",
    "title": "Example",
    "content": "Let’s take a look at some examples, so we can see exactly how it works. The wikipedia article for LZSS has a great example for this, which I’ll use here, and it’s worth a read as an introduction to LZSS. So let’s encode an exceprt of Dr. Seuss’s Green Eggs and Ham with LZSS (credit to Wikipedia for this example). I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! DO WOULD YOU LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM,SAM-I-AM. I DO NOT LIKE GREEN EGGS AND HAM. This text takes up 192 bytes in a typical UTF-8 encoding. Let’s take a look at the LZSS encoded version. I AM SAM. &lt;10,10&gt;SAM I AM. THAT SAM-I-AM! T&lt;15,14&gt;I DO NOT LIKE&lt;29,15&gt; DO WOULD YOU LIKE GREEN EGGS AND HAM? I&lt;69,15&gt;EM,&lt;113,8&gt;.&lt;29,15&gt;GR&lt;64,16&gt;. This encoded, or compressed, version only takes 148 bytes to store (without a magic type to describe the file type), which is a 77% of the original file size, or a compression ratio of 1.3. Not bad! . Analysis . Now let’s take a second understand what’s happening before you start trying to conquer the world with LZSS. As we can see, the “tokens” are reducing the size of the file by referencing pieces of text that are longer than the actual token. Let’s look at the first line: . I AM SAM. &lt;10,10&gt;SAM I AM. The encoder works character by character. On the first character, ‘I’, it checks it’s search buffer to see if it’s already seen an ‘I’. The search buffer is essentially the encoder’s memory, for every character it encodes, it adds it into the search buffer so it can “remember” it. Because it hasn’t seen an ‘I’ already (the search buffer is empty), it just outputs an ‘I’, adds it to the search buffer, and moves to the next character. The next character is ‘ ‘ (a space). The encoder checks the search buffer to see if it’s seen a space before, and it hasn’t so it outputs the space and moves forward. Once it gets to the second space (after “I AM”), the LZ77 starts to come into play. It’s already seen a space before because it’s in the search buffer so it’s ready to output a token, but first it tries to maximize how much text the token is referencing. If it didn’t do this you could imagine that for every character it’s already seen it would output something similar to &lt;5,1&gt;, which is 5 times larger than any character. So once it finds a character that it’s already seen, it moves on to the next character and checks if it’s already seen the next character directly after the previous chracter. Once it finds a sequence of characters that it hasn’t already seen, then it goes back one character to the sequence of characters it’s already seen and prepares the token. Once the token is ready, the difference between LZ77 and LZSS starts to shine. At this point LZ77 simply outputs the token, adds the characters to the search buffer and continues. LZSS does something a little smarter, it will check to see if the size of the outputted token is larger than the text it’s representing. If so, it will output the text it represents, not the token, add the text to the search buffer, and continue. If not, it will output the token, add the text it represents to the search buffer and continue. Coming back to our example, the space character has already been seen, but a space followed by an “S” hasn’t been seen yet (“ S”), so we prepare the token representing the space. The token in our case would be “&lt;3,1&gt;”, which means go back three characters and copy 1 character(s). Next we check to see if our token is longer than our text, and “&lt;3,1&gt;” is indeed longer than “ “, so it wouldn’t make sense to output the token, so we output the space, add it to our search buffer, and continue. This entire process continues until we get to the “I AM SAM. “. At this point we’ve already seen an “I AM SAM. “ but haven’t seen an “I AM SAM. S” so we know our token will represent “I AM SAM. “. Then we check to see if “I AM SAM. “ is longer than “&lt;10,10&gt;”, which it is, so we output the token, add the text to our search buffer and go along. This process continues, encoding tokens and adding text to the search buffer character by character until it’s finished encoding everything. Takeaways . There’s a lot of information to unpack here, but the algorithm at a high level is quite simple: . | Go character by character | Check if it’s seen the character before . | If so, check the next character and prepare a token to be outputted . | If the token is longer than the text it’s representing, don’t output a token | Add the text to the search buffer and continue | . | If not, add the character to the search buffer and continue | . | . It’s important to remember that no matter the outcome, token or no token, the text is always appended to the search buffer so it can always “remember” the text it’s already seen. ",
    "url": "https://go-compression.github.io//algorithms/lzss.html#example",
    "relUrl": "/algorithms/lzss.html#example"
  },"12": {
    "doc": "LZSS",
    "title": "Implementation",
    "content": "Now let’s take a stab at building our very own version so we can understand it more deeply. ",
    "url": "https://go-compression.github.io//algorithms/lzss.html#implementation",
    "relUrl": "/algorithms/lzss.html#implementation"
  },"13": {
    "doc": "LZSS",
    "title": "LZSS",
    "content": " ",
    "url": "https://go-compression.github.io//algorithms/lzss.html",
    "relUrl": "/algorithms/lzss.html"
  },"14": {
    "doc": "Overview of Algorithms",
    "title": "Overview of Algorithms",
    "content": "The following is intended to be a comprehensive list of lossless compression algorithms (in no particular order), however if you feel like an algorithm is missing, please let us know. | Run-length Coding | Range Coding | Lempel-Ziv . | LZ77 | LZ78 | LZSS | LZW | . | Variable-length Coding | Huffman Coding | Arithmetic Coding | Dynamic Markov Compression | FLATE | . For a more complete list, check out these Wikipedia pages on lossless algorithms and lossy algorithms. ",
    "url": "https://go-compression.github.io//algorithms/overview.html",
    "relUrl": "/algorithms/overview.html"
  },"15": {
    "doc": "",
    "title": "",
    "content": "--- layout: default title: Reference nav_order: 8 --- # Reference This page serves to be a reference to topics that aren't directly related to compression, but inveitably come into play. A basic understanding of these concepts are invaluable when building your own implementation or algorithm. ## Bytes, Bits, and Nibbles Stub. ## File Encodings Stub. ## Unix Magic Types Stub. ## Compression Ratio Calculation Stub. https://en.wikipedia.org/wiki/Data_compression_ratio ",
    "url": "https://go-compression.github.io//reference.html",
    "relUrl": "/reference.html"
  }
}
